"""
JSONLæ–‡ä»¶åˆ†å‰²è„šæœ¬ - å¤šçº¿ç¨‹å¹¶å‘ç‰ˆæœ¬

åŠŸèƒ½æè¿°:
    è¿™ä¸ªè„šæœ¬ç”¨äºå°†å¤§å‹JSONLæ–‡ä»¶æŒ‰æŒ‡å®šçš„æ ·æœ¬æ•°é‡ä¸Šé™è¿›è¡Œåˆ†å‰²ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘å¤„ç†ã€‚
    åˆ†å‰²å®Œæˆåä¼šè‡ªåŠ¨åˆ é™¤åŸå§‹æ–‡ä»¶ï¼Œç¡®ä¿æ•°æ®ç›®å½•çš„æ•´æ´ã€‚

ä¸»è¦åŠŸèƒ½:
    1. ğŸš€ å¤šçº¿ç¨‹å¹¶å‘å¤„ç†å¤šä¸ªJSONLæ–‡ä»¶
    2. âœ‚ï¸ æŒ‰ç”¨æˆ·æŒ‡å®šæ ·æœ¬ä¸Šé™åˆ†å‰²æ–‡ä»¶
    3. ğŸ—‘ï¸ è‡ªåŠ¨åˆ é™¤åŸå§‹æ–‡ä»¶
    4. ğŸ“‹ æ™ºèƒ½å‘½åæ–°åˆ†å‰²çš„æ–‡ä»¶
    5. ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º

å·¥ä½œæµç¨‹:
    1. æ‰«ææ•°æ®é›†é…ç½®ï¼Œæ‰¾åˆ°æ‰€æœ‰MetaFilesç›®å½•
    2. å¤šçº¿ç¨‹å¹¶å‘å¤„ç†æ¯ä¸ªJSONLæ–‡ä»¶
    3. æŒ‰æ ·æœ¬ä¸Šé™åˆ†å‰²æ–‡ä»¶
    4. å®‰å…¨åˆ é™¤åŸå§‹æ–‡ä»¶
    5. ç”Ÿæˆå¤„ç†æŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹:
    # åˆ†å‰²æ‰€æœ‰JSONLæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶æœ€å¤š1000ä¸ªæ ·æœ¬
    python scripts/split_jsonl.py \\
        --datasets_yaml configs/my_dataset.yaml \\
        --max_samples 1000 \\
        --workers 8

    # æŒ‡å®šè¾“å‡ºæ–‡ä»¶åå‰ç¼€
    python scripts/split_jsonl.py \\
        --datasets_yaml configs/my_dataset.yaml \\
        --max_samples \\
        --output_prefix "train_split" \\
        --workers 16

æ³¨æ„äº‹é¡¹:
    - ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜æ”¾åˆ†å‰²åçš„æ–‡ä»¶
    - åŸå§‹æ–‡ä»¶ä¼šè¢«åˆ é™¤ï¼Œè¯·æå‰å¤‡ä»½é‡è¦æ•°æ®
    - çº¿ç¨‹æ•°å»ºè®®è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°çš„1-2å€
"""

import os
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import defaultdict
from tqdm import tqdm

from datatool.utils.parallel import post_allocated_multithread
from datatool.logger import log


@post_allocated_multithread
def split_single_jsonl_file(file_info: Dict[str, Any], **kwargs) -> Dict[str, Any]:
    """åˆ†å‰²å•ä¸ªJSONLæ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰"""
    jsonl_file = Path(file_info["jsonl_file"])
    max_samples = file_info["max_samples_per_file"]
    output_prefix = file_info.get("output_prefix", "part")
    thread_id = kwargs.get("thread_id", "unknown")
    
    try:
        log(f"ğŸ”§ çº¿ç¨‹ {thread_id} å¼€å§‹å¤„ç†: {jsonl_file.name}")
        
        # è¯»å–æ‰€æœ‰æ ·æœ¬
        all_samples = []
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    all_samples.append(sample)
                except json.JSONDecodeError as e:
                    log(f"âš ï¸ JSONè§£æé”™è¯¯ {jsonl_file}:{line_idx}: {e}")
                    continue
        
        if not all_samples:
            log(f"âš ï¸ æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡: {jsonl_file}")
            return {
                "status": "skipped",
                "file": str(jsonl_file),
                "reason": "empty_file",
                "thread_id": thread_id
            }
        
        total_samples = len(all_samples)
        
        # å¦‚æœæ ·æœ¬æ•°ä¸è¶…è¿‡é™åˆ¶ï¼Œæ— éœ€åˆ†å‰²
        if total_samples <= max_samples:
            log(f"ğŸ“‹ çº¿ç¨‹ {thread_id} - {jsonl_file.name}: {total_samples} æ ·æœ¬ï¼Œæ— éœ€åˆ†å‰²")
            return {
                "status": "no_split_needed",
                "file": str(jsonl_file),
                "total_samples": total_samples,
                "thread_id": thread_id
            }
        
        # è®¡ç®—éœ€è¦åˆ†å‰²çš„æ–‡ä»¶æ•°
        num_parts = (total_samples + max_samples - 1) // max_samples
        
        # ç”Ÿæˆåˆ†å‰²åçš„æ–‡ä»¶
        output_files = []
        output_dir = jsonl_file.parent
        base_name = jsonl_file.stem
        
        for part_idx in range(num_parts):
            start_idx = part_idx * max_samples
            end_idx = min((part_idx + 1) * max_samples, total_samples)
            part_samples = all_samples[start_idx:end_idx]
            
            # ç”Ÿæˆæ–°æ–‡ä»¶å
            if output_prefix:
                new_filename = f"{output_prefix}_{base_name}_part{part_idx + 1}.jsonl"
            else:
                new_filename = f"{base_name}_part{part_idx + 1}.jsonl"
            
            new_file_path = output_dir / new_filename
            
            # å†™å…¥åˆ†å‰²åçš„æ–‡ä»¶
            with open(new_file_path, 'w', encoding='utf-8') as f:
                for sample in part_samples:
                    f.write(json.dumps(sample, ensure_ascii=False) + '\n')
            
            output_files.append({
                "file": str(new_file_path),
                "samples": len(part_samples)
            })
        
        log(f"âœ… çº¿ç¨‹ {thread_id} - {jsonl_file.name}: åˆ†å‰²ä¸º {num_parts} ä¸ªæ–‡ä»¶")
        
        return {
            "status": "success",
            "original_file": str(jsonl_file),
            "total_samples": total_samples,
            "output_files": output_files,
            "num_parts": num_parts,
            "thread_id": thread_id
        }
        
    except Exception as e:
        return {
            "status": "error",
            "file": str(jsonl_file),
            "message": str(e),
            "thread_id": thread_id
        }


def collect_jsonl_files(yaml_path: str) -> List[Dict[str, Any]]:
    """æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„JSONLæ–‡ä»¶"""
    with open(yaml_path, "r") as f:
        dataset_config = yaml.safe_load(f)
    
    jsonl_files = []
    
    # éå†æ‰€æœ‰æ•°æ®é›†
    for dataset_name, config in dataset_config["Datasets"].items():
        metafile_dir = Path(config["MetaFiles"])
        
        if not metafile_dir.exists():
            log(f"âš ï¸ MetaFilesç›®å½•ä¸å­˜åœ¨: {metafile_dir}")
            continue
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONLæ–‡ä»¶
        dataset_jsonl_files = list(metafile_dir.glob("*.jsonl"))
        
        for jsonl_file in dataset_jsonl_files:
            jsonl_files.append({
                "dataset_name": dataset_name,
                "jsonl_file": str(jsonl_file),
                "metafile_dir": str(metafile_dir)
            })
    
    return jsonl_files


def safe_delete_files(files_to_delete: List[str], num_workers: int = 4) -> Dict[str, Any]:
    """å®‰å…¨åˆ é™¤æ–‡ä»¶åˆ—è¡¨"""
    deleted_count = 0
    failed_deletes = []
    
    log(f"ğŸ—‘ï¸ å¼€å§‹åˆ é™¤ {len(files_to_delete)} ä¸ªåŸå§‹æ–‡ä»¶...")
    
    for file_path in tqdm(files_to_delete, desc="åˆ é™¤åŸå§‹æ–‡ä»¶"):
        try:
            if Path(file_path).exists():
                Path(file_path).unlink()
                deleted_count += 1
            else:
                log(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {file_path}")
        except Exception as e:
            log(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            failed_deletes.append(file_path)
    
    return {
        "deleted_count": deleted_count,
        "failed_deletes": failed_deletes
    }


def split_jsonl_files(yaml_path: str, max_samples_per_file: int, output_prefix: str = None, num_workers: int = 8):
    """ä¸»å‡½æ•°ï¼šåˆ†å‰²JSONLæ–‡ä»¶"""
    yaml_path = Path(yaml_path)
    
    if not yaml_path.exists():
        log(f"âŒ YAMLé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_path}")
        return
    
    log(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
    log(f"  YAMLé…ç½®: {yaml_path}")
    log(f"  æ¯æ–‡ä»¶æœ€å¤§æ ·æœ¬æ•°: {max_samples_per_file}")
    log(f"  è¾“å‡ºå‰ç¼€: {output_prefix or '(æ— )'}")
    log(f"  çº¿ç¨‹æ•°: {num_workers}")
    
    # æ”¶é›†æ‰€æœ‰JSONLæ–‡ä»¶
    log("ğŸ“‹ æ”¶é›†JSONLæ–‡ä»¶...")
    all_jsonl_files = collect_jsonl_files(str(yaml_path))
    
    if not all_jsonl_files:
        log("âŒ æœªæ‰¾åˆ°ä»»ä½•JSONLæ–‡ä»¶")
        return
    
    log(f"ğŸ“Š æ‰¾åˆ° {len(all_jsonl_files)} ä¸ªJSONLæ–‡ä»¶éœ€è¦å¤„ç†")
    
    # å‡†å¤‡å¤„ç†ä»»åŠ¡
    file_tasks = []
    for file_info in all_jsonl_files:
        file_tasks.append({
            "jsonl_file": file_info["jsonl_file"],
            "max_samples_per_file": max_samples_per_file,
            "output_prefix": output_prefix,
            "dataset_name": file_info["dataset_name"]
        })
    
    # å¤šçº¿ç¨‹å¹¶å‘å¤„ç†
    log(f"ğŸš€ å¼€å§‹å¤šçº¿ç¨‹åˆ†å‰² (ä½¿ç”¨ {num_workers} ä¸ªçº¿ç¨‹)...")
    results = split_single_jsonl_file(file_tasks, num_workers=num_workers)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = 0
    no_split_count = 0
    error_count = 0
    skipped_count = 0
    files_to_delete = []
    total_new_files = 0
    total_samples = 0
    
    for result in results:
        status = result["status"]
        if status == "success":
            success_count += 1
            files_to_delete.append(result["original_file"])
            total_new_files += result["num_parts"]
            total_samples += result["total_samples"]
        elif status == "no_split_needed":
            no_split_count += 1
            total_samples += result["total_samples"]
        elif status == "error":
            error_count += 1
            log(f"âŒ å¤„ç†å¤±è´¥: {result['file']} - {result['message']}")
        elif status == "skipped":
            skipped_count += 1
    
    # åˆ é™¤åŸå§‹æ–‡ä»¶
    if files_to_delete:
        delete_result = safe_delete_files(files_to_delete, num_workers)
        log(f"ğŸ—‘ï¸ åˆ é™¤ç»“æœ: æˆåŠŸ {delete_result['deleted_count']} ä¸ªï¼Œå¤±è´¥ {len(delete_result['failed_deletes'])} ä¸ª")
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    log(f"ğŸ‰ å¤„ç†å®Œæˆ!")
    log(f"  âœ… æˆåŠŸåˆ†å‰²: {success_count} ä¸ªæ–‡ä»¶")
    log(f"  ğŸ“‹ æ— éœ€åˆ†å‰²: {no_split_count} ä¸ªæ–‡ä»¶")
    log(f"  â­ï¸ è·³è¿‡å¤„ç†: {skipped_count} ä¸ªæ–‡ä»¶")
    log(f"  âŒ å¤„ç†å¤±è´¥: {error_count} ä¸ªæ–‡ä»¶")
    log(f"  ğŸ“ æ–°å¢æ–‡ä»¶: {total_new_files} ä¸ª")
    log(f"  ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="JSONLæ–‡ä»¶åˆ†å‰²å·¥å…· - å¤šçº¿ç¨‹å¹¶å‘ç‰ˆæœ¬")
    parser.add_argument("--datasets_yaml", required=True, help="æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max_samples", type=int, default=1000, help="æ¯ä¸ªJSONLæ–‡ä»¶çš„æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--output_prefix", default=None, help="è¾“å‡ºæ–‡ä»¶åå‰ç¼€ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--workers", type=int, default=8, help="å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 8)")
    
    args = parser.parse_args()
    
    split_jsonl_files(
        yaml_path=args.datasets_yaml,
        max_samples_per_file=args.max_samples,
        output_prefix=args.output_prefix,
        num_workers=args.workers
    )