"""
æ•°æ®é›†åˆå¹¶è„šæœ¬ - å¤šè¿›ç¨‹å¹¶å‘ç‰ˆæœ¬

åŠŸèƒ½æè¿°:
    è¿™ä¸ªè„šæœ¬ç”¨äºå°†å¤šä¸ªæ•°æ®é›†åˆå¹¶åˆ°ç›®æ ‡æ•°æ®é›†ä¸­ï¼Œæ”¯æŒå¤šè¿›ç¨‹å¹¶å‘å¤„ç†ã€‚
    åªå¤„ç†MetaFileså’ŒMediaFiles

ä¸»è¦åŠŸèƒ½:
    1. ğŸš€ å¤šè¿›ç¨‹å¹¶å‘å¤„ç†æ•°æ®é›†åˆå¹¶
    2. ğŸ“ å¤åˆ¶MetaFileså’ŒMediaFilesåˆ°ç›®æ ‡ä½ç½®
    3. ğŸ“‹ è‡ªåŠ¨æ›´æ–°YAMLé…ç½®æ–‡ä»¶
    4. ğŸ”„ æ”¯æŒæ–°æ•°æ®é›†ç±»åˆ«çš„æ·»åŠ 
    5. ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º

å·¥ä½œæµç¨‹:
    1. åŠ è½½ç›®æ ‡æ•°æ®é›†é…ç½®
    2. éå†è¦åˆå¹¶çš„æ•°æ®é›†é…ç½®æ–‡ä»¶
    3. å¤šè¿›ç¨‹å¹¶å‘å¤åˆ¶æ•°æ®é›†å†…å®¹
    4. æ›´æ–°ç›®æ ‡YAMLé…ç½®æ–‡ä»¶

ä½¿ç”¨ç¤ºä¾‹:
    # å°†tmp.yamlä¸­çš„æ•°æ®é›†å¤åˆ¶åˆ°ç›®æ ‡ç›®å½•
    python merge_dataset.py \\
        --src_dataset_yaml /path/to/tmp.yaml \\
        --dst_root_dir /path/to/target/directory \\
        --num_workers 4

    # åˆå¹¶å¤šä¸ªé…ç½®æ–‡ä»¶åˆ°ç°æœ‰ç›®æ ‡ï¼ˆä½¿ç”¨å¹¶å‘å“ˆå¸Œï¼‰
    python merge_dataset.py \\
        --src_dataset_yaml /path/to/tmp.yaml \\
        --dst_root_dir /path/to/target/directory \\
        --copy_media \\
        --hash_threads 16 \\
        --num_workers 8

æ³¨æ„äº‹é¡¹:
    - ç¡®ä¿ç›®æ ‡ç›®å½•æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´
    - åˆå¹¶å‰å»ºè®®å¤‡ä»½é‡è¦æ•°æ®
    - è¿›ç¨‹æ•°å»ºè®®æ ¹æ®ç£ç›˜I/Oèƒ½åŠ›è°ƒæ•´
"""

import os
import sys
import uuid
import shutil
import yaml
import json
from pathlib import Path
from glob import glob
from tqdm import tqdm

import hashlib
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import threading

from datatool.utils.parallel import post_allocated_multiprocess, post_allocated_multithread
from datatool.logger import log


def simple_copy_dataset(src_dataset, dst_dataset, dataset_name):
    """å°† src_dataset åˆå¹¶åˆ° dst_dataset ä¸­ï¼ˆåªå¤„ç†MetaFileså’ŒMediaFilesï¼‰"""
    src_metadir = Path(src_dataset["MetaFiles"])
    dst_metadir = Path(dst_dataset["MetaFiles"])
    
    # ç¡®ä¿ç›®æ ‡MetaFilesç›®å½•å­˜åœ¨
    dst_metadir.mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶MetaFiles
    if src_metadir.exists():
        # åˆ›å»ºå”¯ä¸€çš„å­ç›®å½•åé¿å…å†²çª
        unique_id = str(uuid.uuid4())[:8]
        copyto_metadir = dst_metadir / f"datatool_merge_{dataset_name}_{unique_id}"
        shutil.copytree(src_metadir, copyto_metadir)
        log(f"âœ… å¤åˆ¶ MetaFiles: {src_metadir} -> {copyto_metadir}")
    else:
        log(f"âš ï¸ æºMetaFilesç›®å½•ä¸å­˜åœ¨: {src_metadir}")
    
    # å¤„ç†MediaFilesï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
    src_dataset_dir = src_metadir.parent
    src_mediadir = src_dataset_dir / "MediaFiles"
    
    if src_mediadir.exists():
        dst_dataset_dir = dst_metadir.parent
        dst_mediadir = dst_dataset_dir / "MediaFiles"
        dst_mediadir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå”¯ä¸€çš„MediaFileså­ç›®å½•
        unique_id = str(uuid.uuid4())[:8]
        copyto_mediadir = dst_mediadir / f"datatool_merge_{dataset_name}_{unique_id}"
        shutil.copytree(src_mediadir, copyto_mediadir)
        log(f"âœ… å¤åˆ¶ MediaFiles: {src_mediadir} -> {copyto_mediadir}")
    else:
        log(f"ğŸ“‹ æºMediaFilesç›®å½•ä¸å­˜åœ¨: {src_mediadir}")


def get_file_hash_fast(file_path):
    """å¿«é€Ÿè·å–æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œä¼˜åŒ–ç‰ˆæœ¬
    - é¦–å…ˆæ¯”è¾ƒæ–‡ä»¶å¤§å°è¿›è¡Œåˆæ­¥å»é‡
    - å¯¹äºå¤§æ–‡ä»¶ï¼Œåªè®¡ç®—æ–‡ä»¶å¤´å’Œå°¾éƒ¨çš„å“ˆå¸Œ
    - å¯¹äºå°æ–‡ä»¶ï¼Œè®¡ç®—å®Œæ•´MD5
    """
    try:
        file_path = Path(file_path)
        file_size = file_path.stat().st_size
        
        # ç©ºæ–‡ä»¶ç›´æ¥è¿”å›ç‰¹æ®Šå“ˆå¸Œ
        if file_size == 0:
            return "empty_file"
        
        # å°æ–‡ä»¶ï¼ˆ<10MBï¼‰è®¡ç®—å®Œæ•´MD5
        if file_size < 10 * 1024 * 1024:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        
        # å¤§æ–‡ä»¶åªè®¡ç®—å¤´éƒ¨ã€ä¸­é—´ã€å°¾éƒ¨çš„å“ˆå¸Œ
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            # è¯»å–æ–‡ä»¶å¤´éƒ¨ 1MB
            head_data = f.read(1024 * 1024)
            hash_md5.update(head_data)
            
            # è¯»å–æ–‡ä»¶ä¸­é—´ 1MB
            if file_size > 2 * 1024 * 1024:
                f.seek(file_size // 2)
                middle_data = f.read(1024 * 1024)
                hash_md5.update(middle_data)
            
            # è¯»å–æ–‡ä»¶å°¾éƒ¨ 1MB
            if file_size > 1024 * 1024:
                f.seek(max(0, file_size - 1024 * 1024))
                tail_data = f.read()
                hash_md5.update(tail_data)
        
        # å°†æ–‡ä»¶å¤§å°ä¹ŸåŠ å…¥å“ˆå¸Œè®¡ç®—ï¼Œå¢åŠ å”¯ä¸€æ€§
        hash_md5.update(str(file_size).encode())
        return f"fast_{hash_md5.hexdigest()}"
        
    except Exception as e:
        log(f"âš ï¸ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
        return None


def get_file_hash(file_path):
    """è·å–æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    return get_file_hash_fast(file_path)


@post_allocated_multithread
def compute_hash_for_file_thread(file_info, **kwargs):
    """å¹¶å‘è®¡ç®—å•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    thread_id = kwargs.get('thread_id', 'unknown')
    
    try:
        src_path = file_info["src_path"]
        file_hash = get_file_hash(src_path)
        
        return {
            "status": "success",
            "src_path": src_path,
            "file_hash": file_hash,
            "media_file": file_info,
            "worker_id": thread_id
        }
    except Exception as e:
        return {
            "status": "error",
            "src_path": file_info.get("src_path", "unknown"),
            "error": str(e),
            "worker_id": thread_id
        }


@post_allocated_multiprocess
def compute_hash_for_file_process(file_info, **kwargs):
    """å¹¶å‘è®¡ç®—å•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰"""
    process_id = kwargs.get('process_id', 'unknown')
    
    try:
        src_path = file_info["src_path"]
        file_hash = get_file_hash(src_path)
        
        return {
            "status": "success",
            "src_path": src_path,
            "file_hash": file_hash,
            "media_file": file_info,
            "worker_id": process_id
        }
    except Exception as e:
        return {
            "status": "error",
            "src_path": file_info.get("src_path", "unknown"),
            "error": str(e),
            "worker_id": process_id
        }


def collect_media_files_from_jsonl(metafiles_dir):
    """ä»MetaFilesç›®å½•çš„jsonlæ–‡ä»¶ä¸­æ”¶é›†æ‰€æœ‰åª’ä½“æ–‡ä»¶è·¯å¾„"""
    metafiles_dir = Path(metafiles_dir)
    media_files = []
    
    if not metafiles_dir.exists():
        log(f"âš ï¸ MetaFilesç›®å½•ä¸å­˜åœ¨: {metafiles_dir}")
        return media_files
    
    # æŸ¥æ‰¾æ‰€æœ‰jsonlæ–‡ä»¶
    jsonl_files = list(metafiles_dir.glob("*.jsonl"))
    
    for jsonl_file in jsonl_files:
        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line_idx, line in enumerate(f):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        item = json.loads(line)
                        
                        # æ”¶é›†å„ç§åª’ä½“æ–‡ä»¶è·¯å¾„
                        for media_type in ["images", "videos", "audios"]:
                            media_paths = item.get(media_type, [])
                            if isinstance(media_paths, list):
                                for media_path in media_paths:
                                    if media_path and Path(media_path).exists():
                                        media_files.append({
                                            "src_path": str(Path(media_path)),
                                            "media_type": media_type,
                                            "suffix": Path(media_path).suffix,
                                            "from_jsonl": str(jsonl_file)
                                        })
                                        
                    except json.JSONDecodeError as e:
                        log(f"âš ï¸ JSONè§£æé”™è¯¯ {jsonl_file}:{line_idx}: {e}")
                        continue
                        
        except Exception as e:
            log(f"âš ï¸ è¯»å–jsonlæ–‡ä»¶å¤±è´¥ {jsonl_file}: {e}")
            continue
    
    return media_files


def copy_media_files_with_dedup(media_files, dst_mediadir, dataset_name, hash_threads=4, use_process=False):
    """å¤åˆ¶åª’ä½“æ–‡ä»¶å¹¶å»é‡ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸¤é˜¶æ®µå»é‡ + å¹¶å‘å“ˆå¸Œï¼‰"""
    dst_mediadir = Path(dst_mediadir)
    dst_mediadir.mkdir(parents=True, exist_ok=True)
    
    # å»é‡å¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬
    unique_files = {}  # {file_hash: file_info}
    media_counters = defaultdict(int)  # {media_type: count}
    copied_files = []
    path_mapping = {}  # {src_path: dst_path} - è·¯å¾„æ˜ å°„è¡¨
    size_groups = defaultdict(list)  # {file_size: [media_files]} - æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
    
    log(f"ğŸ” {dataset_name}: ç¬¬ä¸€é˜¶æ®µ - æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„...")
    # ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„ï¼Œå¿«é€Ÿè¿‡æ»¤
    for media_file in tqdm(media_files, desc=f"åˆ†ç»„{dataset_name}åª’ä½“æ–‡ä»¶"):
        src_path = Path(media_file["src_path"])
        
        if not src_path.exists():
            log(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {src_path}")
            continue
        
        try:
            file_size = src_path.stat().st_size
            media_file["file_size"] = file_size
            size_groups[file_size].append(media_file)
        except Exception as e:
            log(f"âš ï¸ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {src_path}: {e}")
            continue
    
    log(f"ğŸ“Š {dataset_name}: å¤§å°åˆ†ç»„ç»Ÿè®¡ - {len(size_groups)} ä¸ªä¸åŒå¤§å°çš„ç»„")
    
    log(f"ğŸ” {dataset_name}: ç¬¬äºŒé˜¶æ®µ - è®¡ç®—å“ˆå¸Œå»é‡...")
    # ç¬¬äºŒé˜¶æ®µï¼šå¯¹æ¯ä¸ªå¤§å°ç»„å†…çš„æ–‡ä»¶è®¡ç®—å“ˆå¸Œ
    for file_size, media_files_group in tqdm(size_groups.items(), desc=f"å¤„ç†{dataset_name}å¤§å°ç»„"):
        if len(media_files_group) == 1:
            # è¯¥å¤§å°åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œæ— éœ€è®¡ç®—å“ˆå¸Œï¼Œç›´æ¥å¤„ç†
            media_file = media_files_group[0]
            src_path = Path(media_file["src_path"])
            
            media_type = media_file["media_type"]
            media_counters[media_type] += 1
            idx = media_counters[media_type]
            suffix = media_file["suffix"]
            
            # ç”Ÿæˆæ–°çš„æ–‡ä»¶åå’Œè·¯å¾„
            new_filename = f"{idx}{suffix}"
            dst_subdir = dst_mediadir / media_type
            dst_subdir.mkdir(parents=True, exist_ok=True)
            dst_path = dst_subdir / new_filename
            
            # å¤åˆ¶æ–‡ä»¶
            try:
                shutil.copy2(src_path, dst_path)
                
                # ä½¿ç”¨æ–‡ä»¶å¤§å°ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆæ— éœ€è®¡ç®—å“ˆå¸Œï¼‰
                size_hash = f"size_{file_size}_{str(src_path)}"
                unique_files[size_hash] = {
                    "src_path": str(src_path),
                    "dst_path": str(dst_path),
                    "media_type": media_type,
                    "idx": idx,
                    "suffix": suffix
                }
                
                copied_files.append({
                    "src_path": str(src_path),
                    "dst_path": str(dst_path),
                    "media_type": media_type
                })
                
                # è®°å½•è·¯å¾„æ˜ å°„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
                path_mapping[str(src_path)] = str(dst_path)
                
            except Exception as e:
                log(f"âš ï¸ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path} -> {dst_path}: {e}")
                continue
        
        else:
            # è¯¥å¤§å°æœ‰å¤šä¸ªæ–‡ä»¶ï¼Œéœ€è¦è®¡ç®—å“ˆå¸Œå»é‡ï¼ˆä½¿ç”¨å¹¶å‘ï¼‰
            process_hash_group_concurrent_merge(
                media_files_group, 
                media_counters, 
                unique_files, 
                path_mapping, 
                copied_files,
                dst_mediadir, 
                dataset_name,
                max_workers=hash_threads,
                use_process=use_process
            )
    
    log(f"ğŸ“Š {dataset_name} åª’ä½“æ–‡ä»¶å¤„ç†ç»Ÿè®¡:")
    log(f"   åŸå§‹æ–‡ä»¶æ•°: {len(media_files)}")
    log(f"   å»é‡åæ–‡ä»¶æ•°: {len(unique_files)}")
    log(f"   å®é™…å¤åˆ¶æ–‡ä»¶æ•°: {len(copied_files)}")
    for media_type, count in media_counters.items():
        log(f"   {media_type}: {count} ä¸ªæ–‡ä»¶")
    
    return copied_files, path_mapping


def process_hash_group_concurrent_merge(media_files_group, media_counters, unique_files, path_mapping, copied_files, dst_mediadir, dataset_name, max_workers=4, use_process=False):
    """å¹¶å‘å¤„ç†éœ€è¦è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶ç»„ï¼ˆmergeä¸“ç”¨ï¼‰"""
    
    # å‡†å¤‡å“ˆå¸Œè®¡ç®—ä»»åŠ¡
    hash_tasks = []
    for media_file in media_files_group:
        src_path = Path(media_file["src_path"])
        if src_path.exists():
            hash_tasks.append(media_file)
    
    if not hash_tasks:
        return
    
    worker_type = "è¿›ç¨‹" if use_process else "çº¿ç¨‹"
    log(f"ğŸ”§ {dataset_name}: å¹¶å‘è®¡ç®— {len(hash_tasks)} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ (ä½¿ç”¨ {min(max_workers, len(hash_tasks))} ä¸ª{worker_type})...")
    
    # é€‰æ‹©å¹¶å‘è®¡ç®—æ–¹å¼
    if use_process:
        # CPUå¯†é›†å‹ï¼šä½¿ç”¨å¤šè¿›ç¨‹
        hash_results = compute_hash_for_file_process(
            hash_tasks,
            num_workers=min(max_workers, len(hash_tasks))
        )
    else:
        # I/Oå¯†é›†å‹ï¼šä½¿ç”¨å¤šçº¿ç¨‹
        hash_results = compute_hash_for_file_thread(
            hash_tasks,
            num_workers=min(max_workers, len(hash_tasks))
        )
    
    # å¤„ç†å“ˆå¸Œç»“æœï¼Œè¿›è¡Œå»é‡å’Œå¤åˆ¶
    group_hash_map = {}
    for result in hash_results:
        if result["status"] != "success" or not result["file_hash"]:
            continue
        
        file_hash = result["file_hash"]
        media_file = result["media_file"]
        src_path = Path(media_file["src_path"])
        
        if file_hash not in group_hash_map:
            # æ–°çš„å”¯ä¸€æ–‡ä»¶
            media_type = media_file["media_type"]
            media_counters[media_type] += 1
            idx = media_counters[media_type]
            suffix = media_file["suffix"]
            
            # ç”Ÿæˆæ–°çš„æ–‡ä»¶åå’Œè·¯å¾„
            new_filename = f"{idx}{suffix}"
            dst_subdir = dst_mediadir / media_type
            dst_subdir.mkdir(parents=True, exist_ok=True)
            dst_path = dst_subdir / new_filename
            
            # å¤åˆ¶æ–‡ä»¶
            try:
                shutil.copy2(src_path, dst_path)
                
                group_hash_map[file_hash] = {
                    "src_path": str(src_path),
                    "dst_path": str(dst_path),
                    "media_type": media_type,
                    "idx": idx,
                    "suffix": suffix
                }
                
                unique_files[file_hash] = group_hash_map[file_hash]
                
                copied_files.append({
                    "src_path": str(src_path),
                    "dst_path": str(dst_path),
                    "media_type": media_type
                })
                
                # è®°å½•è·¯å¾„æ˜ å°„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
                path_mapping[str(src_path)] = str(dst_path)
                
            except Exception as e:
                log(f"âš ï¸ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path} -> {dst_path}: {e}")
                continue
        else:
            # é‡å¤æ–‡ä»¶ï¼Œä½¿ç”¨å·²æœ‰çš„è·¯å¾„æ˜ å°„
            existing_info = group_hash_map[file_hash]
            path_mapping[str(src_path)] = existing_info["dst_path"]


def update_jsonl_paths(src_metadir, dst_metadir, path_mapping, dataset_name):
    """æ›´æ–°JSONLæ–‡ä»¶ä¸­çš„åª’ä½“è·¯å¾„ä¸ºç»å¯¹è·¯å¾„"""
    src_metadir = Path(src_metadir)
    dst_metadir = Path(dst_metadir)
    
    if not path_mapping:
        log(f"ğŸ“‹ {dataset_name}: æ²¡æœ‰è·¯å¾„æ˜ å°„ï¼Œè·³è¿‡JSONLæ›´æ–°")
        return
    
    log(f"ğŸ”§ {dataset_name}: å¼€å§‹æ›´æ–°JSONLæ–‡ä»¶ä¸­çš„åª’ä½“è·¯å¾„...")
    
    # æŸ¥æ‰¾æ‰€æœ‰jsonlæ–‡ä»¶
    jsonl_files = list(dst_metadir.glob("*.jsonl"))
    
    for jsonl_file in jsonl_files:
        try:
            updated_items = []
            
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line_idx, line in enumerate(f):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        item = json.loads(line)
                        
                        # æ›´æ–°åª’ä½“æ–‡ä»¶è·¯å¾„
                        for media_type in ["images", "videos", "audios"]:
                            if media_type in item:
                                media_paths = item[media_type]
                                if isinstance(media_paths, list):
                                    new_paths = []
                                    for src_path in media_paths:
                                        src_path_str = str(Path(src_path))
                                        if src_path_str in path_mapping:
                                            new_paths.append(path_mapping[src_path_str])
                                        else:
                                            new_paths.append(src_path)  # ä¿ç•™åŸè·¯å¾„
                                    item[media_type] = new_paths
                        
                        updated_items.append(item)
                        
                    except json.JSONDecodeError as e:
                        log(f"âš ï¸ JSONè§£æé”™è¯¯ {jsonl_file}:{line_idx}: {e}")
                        continue
            
            # é‡å†™æ›´æ–°åçš„jsonlæ–‡ä»¶
            with open(jsonl_file, 'w', encoding='utf-8') as f:
                for item in updated_items:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            log(f"âœ… æ›´æ–°JSONLæ–‡ä»¶: {jsonl_file.name} ({len(updated_items)} ä¸ªæ•°æ®é¡¹)")
            
        except Exception as e:
            log(f"âš ï¸ æ›´æ–°JSONLæ–‡ä»¶å¤±è´¥ {jsonl_file}: {e}")
            continue


def process_media_files(src_metadir, dst_dir, dataset_name, copy_media_files=True, hash_threads=4, use_process=False):
    """å¤„ç†åª’ä½“æ–‡ä»¶ï¼šä¼˜å…ˆä½¿ç”¨MediaFilesç›®å½•ï¼Œå¦åˆ™ä»jsonlæ”¶é›†"""
    if not copy_media_files:
        log(f"ğŸ“‹ è·³è¿‡åª’ä½“æ–‡ä»¶å¤åˆ¶: {dataset_name}")
        return [], {}
    
    src_metadir = Path(src_metadir)
    dst_dir = Path(dst_dir)
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨MediaFilesç›®å½•
    src_dataset_dir = src_metadir.parent
    src_mediadir = src_dataset_dir / "MediaFiles"
    
    if src_mediadir.exists():
        # ç›´æ¥å¤åˆ¶MediaFilesç›®å½•
        dst_mediadir = dst_dir / "MediaFiles"
        if dst_mediadir.exists():
            shutil.rmtree(dst_mediadir)
        shutil.copytree(src_mediadir, dst_mediadir)
        log(f"âœ… ç›´æ¥å¤åˆ¶ MediaFiles: {src_mediadir} -> {dst_mediadir}")
        return [], {}  # ç›´æ¥å¤åˆ¶æ—¶æ— éœ€è·¯å¾„æ˜ å°„
    else:
        # ä»jsonlæ–‡ä»¶ä¸­æ”¶é›†åª’ä½“æ–‡ä»¶
        log(f"ğŸ” MediaFilesç›®å½•ä¸å­˜åœ¨ï¼Œä»jsonlæ”¶é›†åª’ä½“æ–‡ä»¶: {dataset_name}")
        media_files = collect_media_files_from_jsonl(src_metadir)
        
        if media_files:
            dst_mediadir = dst_dir / "MediaFiles"
            copied_files, path_mapping = copy_media_files_with_dedup(media_files, dst_mediadir, dataset_name, hash_threads, use_process)
            return copied_files, path_mapping
        else:
            log(f"ğŸ“‹ æœªæ‰¾åˆ°åª’ä½“æ–‡ä»¶: {dataset_name}")
            return [], {}


@post_allocated_multiprocess
def process_dataset(data, **kwargs):
    """å¤„ç†å•ä¸ªæ•°æ®é›†çš„åˆå¹¶ï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰"""
    args = kwargs["args"]
    dst_dataset_config = kwargs.get("dst_dataset_config", {})
    process_id = kwargs.get("process_id", "unknown")
    
    dataset_name, config = data
    
    try:
        log(f"ğŸ”§ è¿›ç¨‹ {process_id} å¼€å§‹å¤„ç†: {dataset_name}")
        
        # ç¡®å®šç›®æ ‡ç›®å½•
        dst_root_dir = Path(args.dst_root_dir)
        dst_root_dir.mkdir(parents=True, exist_ok=True)
        
        # ç›®æ ‡æ•°æ®é›†ç›®å½•
        dst_dir = dst_root_dir / dataset_name
        dst_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶MetaFiles
        src_metadir = Path(config["MetaFiles"])
        dst_metadir = dst_dir / "MetaFiles"
        
        if src_metadir.exists():
            if dst_metadir.exists():
                shutil.rmtree(dst_metadir)
            shutil.copytree(src_metadir, dst_metadir)
            log(f"âœ… è¿›ç¨‹ {process_id} - {dataset_name}: å¤åˆ¶ MetaFiles å®Œæˆ")
        else:
            log(f"âš ï¸ è¿›ç¨‹ {process_id} - {dataset_name}: MetaFilesç›®å½•ä¸å­˜åœ¨")
            return {
                "status": "error",
                "dataset": dataset_name,
                "error": "MetaFilesç›®å½•ä¸å­˜åœ¨",
                "process_id": process_id
            }
        
        # å¤„ç†åª’ä½“æ–‡ä»¶
        copied_media_files = []
        path_mapping = {}
        if args.copy_media_files:
            copied_media_files, path_mapping = process_media_files(
                src_metadir, dst_dir, dataset_name, args.copy_media_files, args.hash_threads, args.use_process
            )
            
            # æ›´æ–°JSONLæ–‡ä»¶ä¸­çš„è·¯å¾„ï¼ˆå¦‚æœæœ‰è·¯å¾„æ˜ å°„ï¼‰
            if path_mapping:
                update_jsonl_paths(src_metadir, dst_metadir, path_mapping, dataset_name)
        
        return {
            "status": "success",
            "dataset": dataset_name,
            "action": "created_new",
            "dst_dir": str(dst_dir),
            "sample_nums": config.get("sample_nums", 0),
            "copied_media_files": len(copied_media_files),
            "process_id": process_id
        }
                
    except Exception as e:
        log(f"âŒ è¿›ç¨‹ {process_id} - {dataset_name}: å¤„ç†å¤±è´¥ - {e}")
        return {
            "status": "error",
            "dataset": dataset_name,
            "error": str(e),
            "process_id": process_id
        }


def update_target_yaml(dst_yaml_path, results, dst_root_dir):
    """æ›´æ–°ç›®æ ‡YAMLé…ç½®æ–‡ä»¶"""
    dst_yaml_path = Path(dst_yaml_path)
    dst_root_dir = Path(dst_root_dir)
    
    # ç”Ÿæˆæ–°çš„é…ç½®
    new_config = {
        "DataDir": str(dst_root_dir),
        "Datasets": {},
        "TotalSampleNums": 0
    }
    
    total_samples = 0
    
    # åŸºäºå®é™…å¤åˆ¶ç»“æœæ›´æ–°é…ç½®
    for result in results:
        if result["status"] == "success":
            dataset_name = result["dataset"]
            sample_nums = result.get("sample_nums", 0)
            
            new_config["Datasets"][dataset_name] = {
                "MetaFiles": str(dst_root_dir / dataset_name / "MetaFiles"),
                "sample_nums": sample_nums
            }
            total_samples += sample_nums
    
    new_config["TotalSampleNums"] = total_samples
    
    # ä¿å­˜æ–°é…ç½®
    with open(dst_yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(new_config, f, default_flow_style=False, allow_unicode=True)
    
    log(f"ğŸ“‹ æ›´æ–°é…ç½®æ–‡ä»¶: {dst_yaml_path}")
    log(f"ğŸ“Š æ€»æ•°æ®é›†æ•°: {len(new_config['Datasets'])}")
    log(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")


def merge_datasets(src_yaml_path, dst_root_dir, dst_yaml_path=None, copy_media_files=True, hash_threads=16, use_process=False, num_workers=4):
    """ä¸»å‡½æ•°ï¼šåˆå¹¶æ•°æ®é›†"""
    src_yaml_path = Path(src_yaml_path)
    dst_root_dir = Path(dst_root_dir)
    
    log(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
    log(f"  æºé…ç½®æ–‡ä»¶: {src_yaml_path}")
    log(f"  ç›®æ ‡æ ¹ç›®å½•: {dst_root_dir}")
    log(f"  ç›®æ ‡é…ç½®æ–‡ä»¶: {dst_yaml_path or '(è‡ªåŠ¨ç”Ÿæˆ)'}")
    log(f"  å¤åˆ¶åª’ä½“æ–‡ä»¶: {copy_media_files}")
    log(f"  å“ˆå¸Œè®¡ç®—çº¿ç¨‹æ•°: {hash_threads}")
    log(f"  ä½¿ç”¨å¤šè¿›ç¨‹å“ˆå¸Œ: {use_process}")
    log(f"  è¿›ç¨‹æ•°: {num_workers}")
    
    # åŠ è½½æºé…ç½®
    with open(src_yaml_path, 'r', encoding='utf-8') as f:
        src_config = yaml.safe_load(f)
    
    # åŠ è½½ç›®æ ‡é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    dst_config = {}
    if dst_yaml_path and Path(dst_yaml_path).exists():
        with open(dst_yaml_path, 'r', encoding='utf-8') as f:
            dst_config = yaml.safe_load(f)
        log(f"ğŸ“‹ åŠ è½½ç°æœ‰ç›®æ ‡é…ç½®: {dst_yaml_path}")
    
    # å‡†å¤‡å¤„ç†ä»»åŠ¡
    all_data = []
    for dataset_name, config in src_config["Datasets"].items():
        all_data.append([dataset_name, config])
    
    if not all_data:
        log("âŒ æºé…ç½®ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›†")
        return
    
    log(f"ğŸ“Š æ‰¾åˆ° {len(all_data)} ä¸ªæ•°æ®é›†éœ€è¦å¤„ç†")
    
    # å‡†å¤‡å‚æ•°
    class Args:
        def __init__(self):
            self.dst_root_dir = str(dst_root_dir)
            self.copy_media_files = copy_media_files
            self.hash_threads = hash_threads
    
    args = Args()
    
    # å¤šè¿›ç¨‹å¤„ç†
    log(f"ğŸš€ å¼€å§‹å¤šè¿›ç¨‹å¤„ç† (ä½¿ç”¨ {num_workers} ä¸ªè¿›ç¨‹)...")
    results = process_dataset(
        all_data,
        num_workers=num_workers,
        dst_dataset_config=dst_config,
        args=args
    )
    
    # ç»Ÿè®¡ç»“æœ
    success_count = 0
    error_count = 0
    skipped_count = 0
    total_samples = 0
    
    for result in results:
        if result["status"] == "success":
            success_count += 1
            total_samples += result.get("sample_nums", 0)
        elif result["status"] == "error":
            error_count += 1
        elif result["status"] == "skipped":
            skipped_count += 1
    
    # ç”Ÿæˆç›®æ ‡é…ç½®æ–‡ä»¶
    if not dst_yaml_path:
        dst_yaml_path = dst_root_dir / "dataset_config.yaml"
    
    update_target_yaml(dst_yaml_path, results, dst_root_dir)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    log(f"ğŸ‰ å¤„ç†å®Œæˆ!")
    log(f"  âœ… æˆåŠŸå¤„ç†: {success_count} ä¸ªæ•°æ®é›†")
    log(f"  âŒ å¤„ç†å¤±è´¥: {error_count} ä¸ªæ•°æ®é›†")
    log(f"  â­ï¸ è·³è¿‡å¤„ç†: {skipped_count} ä¸ªæ•°æ®é›†")
    log(f"  ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")
    log(f"  ğŸ“ ç›®æ ‡ç›®å½•: {dst_root_dir}")
    log(f"  ğŸ“‹ é…ç½®æ–‡ä»¶: {dst_yaml_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®é›†åˆå¹¶å·¥å…· - å¤šè¿›ç¨‹å¹¶å‘ç‰ˆæœ¬")
    parser.add_argument("--src_dataset_yaml", required=True, 
                        help="æºæ•°æ®é›†YAMLé…ç½®æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: tmp.yaml)")
    parser.add_argument("--dst_root_dir", required=True,
                        help="ç›®æ ‡æ ¹ç›®å½•è·¯å¾„ (ä¾‹å¦‚: /workspace/tmp_data)")
    parser.add_argument("--dst_dataset_yaml", default=None,
                        help="ç›®æ ‡æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œå¦‚æœä¸æŒ‡å®šå°†è‡ªåŠ¨ç”Ÿæˆ)")
    parser.add_argument("--copy_media", action="store_true",
                        help="æ˜¯å¦å¤åˆ¶åª’ä½“æ–‡ä»¶ (é»˜è®¤: True)")
    parser.add_argument("--hash_threads", type=int, default=16,
                        help="å“ˆå¸Œè®¡ç®—å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 16)")
    parser.add_argument("--use_process", action="store_true",
                        help="ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œå“ˆå¸Œè®¡ç®—ï¼ˆCPUå¯†é›†å‹ä¼˜åŒ–ï¼‰")
    parser.add_argument("--num_workers", type=int, default=4,
                        help="å¹¶å‘è¿›ç¨‹æ•° (é»˜è®¤: 4)")
    
    args = parser.parse_args()
    
    merge_datasets(
        src_yaml_path=args.src_dataset_yaml,
        dst_root_dir=args.dst_root_dir,
        dst_yaml_path=args.dst_dataset_yaml,
        copy_media_files=args.copy_media,
        hash_threads=args.hash_threads,
        use_process=args.use_process,
        num_workers=args.num_workers
    )