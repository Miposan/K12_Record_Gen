"""
æ•°æ®é›†é‡å»ºè„šæœ¬ - ç¬¦åˆåŸæœ‰é€»è¾‘ç»“æ„ï¼ˆä¿®å¤ç‰ˆï¼‰

åŠŸèƒ½æè¿°:
    è¿™ä¸ªè„šæœ¬ç”¨äºé‡å»ºå’Œæ•´åˆæ•°æ®é›†ï¼Œè¾“å…¥ä¸€ä¸ªåŒ…å«å¤šä¸ªæ•°æ®é›†çš„ç›®å½•è·¯å¾„ï¼Œ
    è‡ªåŠ¨éå†å„æ•°æ®é›†çš„MetaFilesç›®å½•ï¼Œæ”¶é›†åª’ä½“æ–‡ä»¶å¹¶å»é‡é‡å»ºã€‚

ä¸»è¦åŠŸèƒ½:
    1. ğŸš€ å¤šè¿›ç¨‹å¹¶å‘å¤„ç†æ•°æ®é›†é‡å»º
    2. ğŸ“ è‡ªåŠ¨éå†ç›®å½•ä¸‹çš„æ•°æ®é›†MetaFiles
    3. ğŸ”„ ä»jsonlä¸­æ”¶é›†åª’ä½“æ–‡ä»¶å¹¶å»é‡
    4. ğŸ“¦ é‡æ–°ç»„ç»‡MediaFilesç›®å½•ç»“æ„
    5. ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º

å·¥ä½œæµç¨‹:
    1. æ‰«æè¾“å…¥ç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰æ•°æ®é›†çš„MetaFiles
    2. å¹¶å‘å¤„ç†æ¯ä¸ªæ•°æ®é›†
    3. ä»jsonlæ”¶é›†åª’ä½“æ–‡ä»¶è·¯å¾„å¹¶å»é‡
    4. å¤åˆ¶åˆ°æ–°çš„MediaFiles/{media_type}/{idx}.{suffix}ç»“æ„
    5. æ›´æ–°jsonlä¸­çš„è·¯å¾„å¼•ç”¨
    6. ç”Ÿæˆæ–°çš„é…ç½®æ–‡ä»¶

ä½¿ç”¨ç¤ºä¾‹:
    # é‡å»ºæ•°æ®é›†
    python reconstruct_dataset.py \\
        --save_dir /workspace/tmp_data_reconstructed \\
        --datasets_dir /workspace/source_datasets \\
        --num_workers 8
    
    # ä½¿ç”¨è‡ªå®šä¹‰MetaFilesç›®å½•å
    python reconstruct_dataset.py \\
        --save_dir /workspace/tmp_data_reconstructed \\
        --datasets_dir /workspace/source_datasets \\
        --metafiles_name CustomMetaFiles \\
        --num_workers 8
    
    # å¤åˆ¶åª’ä½“æ–‡ä»¶ï¼ˆä½¿ç”¨å¹¶å‘å“ˆå¸Œè®¡ç®—ï¼‰
    python reconstruct_dataset.py \\
        --save_dir /workspace/tmp_data_reconstructed \\
        --datasets_dir /workspace/source_datasets \\
        --copy_media \\
        --hash_threads 8 \\
        --num_workers 8
"""

import os
import sys
import uuid
import shutil
import yaml
import json
import hashlib
from pathlib import Path
from glob import glob
from tqdm import tqdm
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import threading

from datatool.utils.parallel import post_allocated_multiprocess, post_allocated_multithread
from datatool.logger import log


def get_file_hash_fast(file_path):
    """å¿«é€Ÿè·å–æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œä¼˜åŒ–ç‰ˆæœ¬
    - é¦–å…ˆæ¯”è¾ƒæ–‡ä»¶å¤§å°è¿›è¡Œåˆæ­¥å»é‡
    - å¯¹äºå¤§æ–‡ä»¶ï¼Œåªè®¡ç®—æ–‡ä»¶å¤´å’Œå°¾éƒ¨çš„å“ˆå¸Œ
    - å¯¹äºå°æ–‡ä»¶ï¼Œè®¡ç®—å®Œæ•´MD5
    """
    try:
        file_path = Path(file_path)
        file_size = file_path.stat().st_size
        
        # ç©ºæ–‡ä»¶ç›´æ¥è¿”å›ç‰¹æ®Šå“ˆå¸Œ
        if file_size == 0:
            return "empty_file"
        
        # å°æ–‡ä»¶ï¼ˆ<10MBï¼‰è®¡ç®—å®Œæ•´MD5
        if file_size < 10 * 1024 * 1024:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        
        # å¤§æ–‡ä»¶åªè®¡ç®—å¤´éƒ¨ã€ä¸­é—´ã€å°¾éƒ¨çš„å“ˆå¸Œ
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            # è¯»å–æ–‡ä»¶å¤´éƒ¨ 1MB
            head_data = f.read(1024 * 1024)
            hash_md5.update(head_data)
            
            # è¯»å–æ–‡ä»¶ä¸­é—´ 1MB
            if file_size > 2 * 1024 * 1024:
                f.seek(file_size // 2)
                middle_data = f.read(1024 * 1024)
                hash_md5.update(middle_data)
            
            # è¯»å–æ–‡ä»¶å°¾éƒ¨ 1MB
            if file_size > 1024 * 1024:
                f.seek(max(0, file_size - 1024 * 1024))
                tail_data = f.read()
                hash_md5.update(tail_data)
        
        # å°†æ–‡ä»¶å¤§å°ä¹ŸåŠ å…¥å“ˆå¸Œè®¡ç®—ï¼Œå¢åŠ å”¯ä¸€æ€§
        hash_md5.update(str(file_size).encode())
        return f"fast_{hash_md5.hexdigest()}"
        
    except Exception as e:
        log(f"âš ï¸ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
        return None


def get_file_hash(file_path):
    """è·å–æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    return get_file_hash_fast(file_path)


@post_allocated_multithread
def compute_hash_for_file_thread(file_info, **kwargs):
    """å¹¶å‘è®¡ç®—å•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    thread_id = kwargs.get('thread_id', 'unknown')
    
    try:
        src_path = file_info["src_path"]
        file_hash = get_file_hash(src_path)
        
        return {
            "status": "success",
            "src_path": src_path,
            "file_hash": file_hash,
            "media_file": file_info,
            "worker_id": thread_id
        }
    except Exception as e:
        return {
            "status": "error",
            "src_path": file_info.get("src_path", "unknown"),
            "error": str(e),
            "worker_id": thread_id
        }


@post_allocated_multiprocess 
def compute_hash_for_file_process(file_info, **kwargs):
    """å¹¶å‘è®¡ç®—å•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰"""
    process_id = kwargs.get('process_id', 'unknown')
    
    try:
        src_path = file_info["src_path"]
        file_hash = get_file_hash(src_path)
        
        return {
            "status": "success",
            "src_path": src_path,
            "file_hash": file_hash,
            "media_file": file_info,
            "worker_id": process_id
        }
    except Exception as e:
        return {
            "status": "error",
            "src_path": file_info.get("src_path", "unknown"),
            "error": str(e),
            "worker_id": process_id
        }


def process_hash_group_concurrent(media_files_group, media_counters, unique_files, path_mapping, media_save_dir, dataset_name, max_workers=4, use_process=False):
    """å¹¶å‘å¤„ç†éœ€è¦è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶ç»„"""
    
    # å‡†å¤‡å“ˆå¸Œè®¡ç®—ä»»åŠ¡
    hash_tasks = []
    for media_file in media_files_group:
        src_path = Path(media_file["src_path"])
        if src_path.exists():
            hash_tasks.append(media_file)
    
    if not hash_tasks:
        return
    
    worker_type = "è¿›ç¨‹" if use_process else "çº¿ç¨‹"
    log(f"ğŸ”§ {dataset_name}: å¹¶å‘è®¡ç®— {len(hash_tasks)} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ (ä½¿ç”¨ {min(max_workers, len(hash_tasks))} ä¸ª{worker_type})...")
    
    # é€‰æ‹©å¹¶å‘è®¡ç®—æ–¹å¼
    if use_process:
        # CPUå¯†é›†å‹ï¼šä½¿ç”¨å¤šè¿›ç¨‹
        hash_results = compute_hash_for_file_process(
            hash_tasks,
            num_workers=min(max_workers, len(hash_tasks))
        )
    else:
        # I/Oå¯†é›†å‹ï¼šä½¿ç”¨å¤šçº¿ç¨‹
        hash_results = compute_hash_for_file_thread(
            hash_tasks,
            num_workers=min(max_workers, len(hash_tasks))
        )
    
    # å¤„ç†å“ˆå¸Œç»“æœï¼Œè¿›è¡Œå»é‡å’Œå¤åˆ¶
    group_hash_map = {}
    for result in hash_results:
        if result["status"] != "success" or not result["file_hash"]:
            continue
        
        file_hash = result["file_hash"]
        media_file = result["media_file"]
        src_path = Path(media_file["src_path"])
        
        if file_hash not in group_hash_map:
            # æ–°çš„å”¯ä¸€æ–‡ä»¶
            media_type = media_file["media_type"]
            media_counters[media_type] += 1
            idx = media_counters[media_type]
            suffix = media_file["suffix"]
            
            # ç”Ÿæˆæ–°çš„æ–‡ä»¶åå’Œè·¯å¾„
            new_filename = f"{idx}{suffix}"
            dst_subdir = Path(media_save_dir) / media_type
            dst_subdir.mkdir(parents=True, exist_ok=True)
            dst_path = dst_subdir / new_filename
            
            # å¤åˆ¶æ–‡ä»¶
            try:
                shutil.copy2(src_path, dst_path)
                
                group_hash_map[file_hash] = {
                    "src_path": str(src_path),
                    "dst_path": str(dst_path),
                    "media_type": media_type,
                    "idx": idx,
                    "suffix": suffix
                }
                
                unique_files[file_hash] = group_hash_map[file_hash]
                
                # è®°å½•è·¯å¾„æ˜ å°„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
                path_mapping[str(src_path)] = str(dst_path)
                
            except Exception as e:
                log(f"âš ï¸ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path} -> {dst_path}: {e}")
                continue
        else:
            # é‡å¤æ–‡ä»¶ï¼Œä½¿ç”¨å·²æœ‰çš„è·¯å¾„æ˜ å°„
            existing_info = group_hash_map[file_hash]
            path_mapping[str(src_path)] = existing_info["dst_path"]


@post_allocated_multiprocess
def process_single_jsonl_file(jsonl_file_info, **kwargs):
    """å¤„ç†å•ä¸ªjsonlæ–‡ä»¶ï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰"""
    jsonl_file, dataset_name, meta_save_dir, media_save_dir = jsonl_file_info
    process_id = kwargs.get('process_id', 0)
    
    try:
        log(f"ğŸ”§ è¿›ç¨‹ {process_id} å¼€å§‹å¤„ç†: {Path(jsonl_file).name}")
        
        # è¯»å–jsonlæ•°æ®
        jsonl_data = []
        media_files = []
        
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                line = line.strip()
                if not line:
                    continue
                try:
                    item = json.loads(line)
                    item_id = item.get("id", f"item_{line_idx}")
                    
                    # ä¿å­˜åŸå§‹æ•°æ®
                    jsonl_data.append({
                        "data": item,
                        "line_idx": line_idx,
                        "src_file": str(jsonl_file)
                    })
                    
                    # æ”¶é›†åª’ä½“æ–‡ä»¶è·¯å¾„
                    for media_type in ["images", "videos", "audios"]:
                        media_paths = item.get(media_type, [])
                        if isinstance(media_paths, list):
                            for idx, media_path in enumerate(media_paths):
                                if media_path and Path(media_path).exists():
                                    media_files.append({
                                        "item_id": item_id,
                                        "line_idx": line_idx,
                                        "media_type": media_type,
                                        "media_idx": idx,
                                        "src_path": str(Path(media_path)),
                                        "suffix": Path(media_path).suffix,
                                        "from_jsonl": str(jsonl_file)
                                    })
                                    
                except json.JSONDecodeError as e:
                    log(f"âš ï¸ JSONè§£æé”™è¯¯ {jsonl_file}:{line_idx}: {e}")
                    continue
        
        return {
            "status": "success",
            "jsonl_file": str(jsonl_file),
            "dataset_name": dataset_name,
            "jsonl_data": jsonl_data,
            "media_files": media_files,
            "sample_count": len(jsonl_data),
            "process_id": process_id
        }
        
    except Exception as e:
        log(f"âŒ è¿›ç¨‹ {process_id} å¤„ç†æ–‡ä»¶å¤±è´¥ {jsonl_file}: {e}")
        return {
            "status": "error",
            "jsonl_file": str(jsonl_file),
            "error": str(e),
            "process_id": process_id
        }


def deduplicate_and_rebuild_dataset(dataset_results, meta_save_dir, media_save_dir, dataset_name, copy_media_files=True, hash_threads=4, use_process=False):
    """å»é‡å¹¶é‡å»ºå•ä¸ªæ•°æ®é›†"""
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_jsonl_data = []
    all_media_files = []
    
    for result in dataset_results:
        if result["status"] == "success":
            all_jsonl_data.extend(result["jsonl_data"])
            all_media_files.extend(result["media_files"])
    
    if not all_jsonl_data:
        log(f"âš ï¸ {dataset_name}: æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®")
        return 0
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤åˆ¶åª’ä½“æ–‡ä»¶
    if not copy_media_files or media_save_dir is None:
        log(f"ğŸ“‹ {dataset_name}: è·³è¿‡åª’ä½“æ–‡ä»¶å¤åˆ¶ï¼Œä»…å¤„ç†MetaFiles")
        
        # ä»…ä¿å­˜jsonlæ•°æ®ï¼Œä¸æ›´æ–°è·¯å¾„
        updated_data = []
        for item_info in all_jsonl_data:
            updated_data.append(item_info["data"])
        
        # ä¿å­˜jsonlæ–‡ä»¶
        jsonl_path = os.path.join(meta_save_dir, f"{dataset_name}.jsonl")
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for item in updated_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        log(f"ğŸ“ ä¿å­˜jsonl: {jsonl_path} ({len(updated_data)} ä¸ªæ•°æ®é¡¹)")
        return len(updated_data)
    
    log(f"ğŸ” {dataset_name}: å¼€å§‹åª’ä½“æ–‡ä»¶å»é‡å¤„ç†...")
    
    # å»é‡å¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬
    unique_files = {}  # {file_hash: file_info}
    path_mapping = {}  # {src_path: new_rel_path}
    media_counters = defaultdict(int)  # {media_type: count}
    size_groups = defaultdict(list)  # {file_size: [media_files]} - æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
    
    log(f"ğŸ” {dataset_name}: ç¬¬ä¸€é˜¶æ®µ - æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„...")
    # ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„ï¼Œå¿«é€Ÿè¿‡æ»¤
    for media_file in tqdm(all_media_files, desc=f"åˆ†ç»„{dataset_name}åª’ä½“æ–‡ä»¶"):
        src_path = Path(media_file["src_path"])
        
        if not src_path.exists():
            log(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {src_path}")
            continue
        
        try:
            file_size = src_path.stat().st_size
            media_file["file_size"] = file_size
            size_groups[file_size].append(media_file)
        except Exception as e:
            log(f"âš ï¸ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {src_path}: {e}")
            continue
    
    log(f"ğŸ“Š {dataset_name}: å¤§å°åˆ†ç»„ç»Ÿè®¡ - {len(size_groups)} ä¸ªä¸åŒå¤§å°çš„ç»„")
    
    log(f"ğŸ” {dataset_name}: ç¬¬äºŒé˜¶æ®µ - è®¡ç®—å“ˆå¸Œå»é‡...")
    # ç¬¬äºŒé˜¶æ®µï¼šå¯¹æ¯ä¸ªå¤§å°ç»„å†…çš„æ–‡ä»¶è®¡ç®—å“ˆå¸Œ
    for file_size, media_files_group in tqdm(size_groups.items(), desc=f"å¤„ç†{dataset_name}å¤§å°ç»„"):
        if len(media_files_group) == 1:
            # è¯¥å¤§å°åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œæ— éœ€è®¡ç®—å“ˆå¸Œï¼Œç›´æ¥å¤„ç†
            media_file = media_files_group[0]
            src_path = Path(media_file["src_path"])
            
            media_type = media_file["media_type"]
            media_counters[media_type] += 1
            idx = media_counters[media_type]
            suffix = media_file["suffix"]
            
            # ç”Ÿæˆæ–°çš„æ–‡ä»¶åå’Œè·¯å¾„
            new_filename = f"{idx}{suffix}"
            dst_subdir = Path(media_save_dir) / media_type
            dst_subdir.mkdir(parents=True, exist_ok=True)
            dst_path = dst_subdir / new_filename
            
            # å¤åˆ¶æ–‡ä»¶
            try:
                shutil.copy2(src_path, dst_path)
                
                # ä½¿ç”¨æ–‡ä»¶å¤§å°ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆæ— éœ€è®¡ç®—å“ˆå¸Œï¼‰
                size_hash = f"size_{file_size}_{str(src_path)}"
                unique_files[size_hash] = {
                    "src_path": str(src_path),
                    "dst_path": str(dst_path),
                    "media_type": media_type,
                    "idx": idx,
                    "suffix": suffix
                }
                
                # è®°å½•è·¯å¾„æ˜ å°„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
                path_mapping[str(src_path)] = str(dst_path)
                
            except Exception as e:
                log(f"âš ï¸ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path} -> {dst_path}: {e}")
                continue
        
        else:
            # è¯¥å¤§å°æœ‰å¤šä¸ªæ–‡ä»¶ï¼Œéœ€è¦è®¡ç®—å“ˆå¸Œå»é‡ï¼ˆä½¿ç”¨å¹¶å‘ï¼‰
            process_hash_group_concurrent(
                media_files_group, 
                media_counters, 
                unique_files, 
                path_mapping, 
                media_save_dir, 
                dataset_name,
                max_workers=hash_threads,
                use_process=use_process
            )
    
    log(f"ğŸ“Š {dataset_name} åª’ä½“æ–‡ä»¶å¤„ç†ç»Ÿè®¡:")
    log(f"   åŸå§‹æ–‡ä»¶æ•°: {len(all_media_files)}")
    log(f"   å»é‡åæ–‡ä»¶æ•°: {len(unique_files)}")
    log(f"   å®é™…å¤åˆ¶æ–‡ä»¶æ•°: {len(unique_files)}")
    for media_type, count in media_counters.items():
        log(f"   {media_type}: {count} ä¸ªæ–‡ä»¶")
    
    # æ›´æ–°jsonlæ•°æ®ä¸­çš„è·¯å¾„
    updated_data = []
    for item_info in all_jsonl_data:
        item = item_info["data"].copy()
        
        # æ›´æ–°åª’ä½“æ–‡ä»¶è·¯å¾„
        for media_type in ["images", "videos", "audios"]:
            if media_type in item:
                media_paths = item[media_type]
                if isinstance(media_paths, list):
                    new_paths = []
                    for src_path in media_paths:
                        src_path_str = str(Path(src_path))
                        if src_path_str in path_mapping:
                            new_paths.append(path_mapping[src_path_str])
                        else:
                            log(f"âš ï¸ æ‰¾ä¸åˆ°è·¯å¾„æ˜ å°„: {src_path}")
                            new_paths.append(src_path)  # ä¿ç•™åŸè·¯å¾„
                    item[media_type] = new_paths
        
        updated_data.append(item)
    
    # ä¿å­˜æ›´æ–°åçš„jsonlæ–‡ä»¶
    jsonl_path = os.path.join(meta_save_dir, f"{dataset_name}.jsonl")
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for item in updated_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    log(f"ğŸ“ ä¿å­˜æ›´æ–°åçš„jsonl: {jsonl_path} ({len(updated_data)} ä¸ªæ•°æ®é¡¹)")
    return len(updated_data)


def stat_sample_nums(meta_save_dir):
    """ç»Ÿè®¡æ ·æœ¬æ•°é‡"""
    total_samples = 0
    jsonl_files = glob(os.path.join(meta_save_dir, "*.jsonl"))
    
    for jsonl_file in jsonl_files:
        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        total_samples += 1
        except Exception as e:
            log(f"âš ï¸ ç»Ÿè®¡æ–‡ä»¶å¤±è´¥ {jsonl_file}: {e}")
    
    return total_samples


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®é›†é‡å»ºå·¥å…· - ç›®å½•è¾“å…¥ç‰ˆæœ¬")
    parser.add_argument("--save_dir", type=str, required=True,
                        help="é‡å»ºæ•°æ®é›†çš„ä¿å­˜ç›®å½•")
    parser.add_argument("--datasets_dir", type=str, required=True,
                        help="æºæ•°æ®é›†ç›®å½•è·¯å¾„ï¼ˆåŒ…å«å¤šä¸ªæ•°æ®é›†å­ç›®å½•ï¼‰")
    parser.add_argument("--save_yaml", type=str, default=None,
                        help="é‡å»ºæ•°æ®é›†çš„é…ç½®æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--metafiles_name", type=str, default="MetaFiles",
                        help="MetaFilesç›®å½•åç§° (é»˜è®¤: MetaFiles)")
    parser.add_argument("--copy_media", action="store_true", default=False,
                        help="æ˜¯å¦å¤åˆ¶åª’ä½“æ–‡ä»¶ (é»˜è®¤: Falseï¼Œä¸å¤åˆ¶)")
    parser.add_argument("--hash_threads", type=int, default=16,
                        help="å“ˆå¸Œè®¡ç®—å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 16)")
    parser.add_argument("--use_process", action="store_true",
                        help="ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œå“ˆå¸Œè®¡ç®—ï¼ˆCPUå¯†é›†å‹ä¼˜åŒ–ï¼‰")
    parser.add_argument("--num_workers", type=int, default=8,
                        help="å¹¶å‘è¿›ç¨‹æ•°")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    datasets_dir = Path(args.datasets_dir)
    if not datasets_dir.exists():
        log(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {datasets_dir}")
        sys.exit(1)
    
    log(f"ğŸ”§ å¼€å§‹é‡å»ºæ•°æ®é›†")
    log(f"  æºç›®å½•: {datasets_dir}")
    log(f"  ç›®æ ‡ç›®å½•: {args.save_dir}")
    log(f"  MetaFilesç›®å½•å: {args.metafiles_name}")
    log(f"  å¤åˆ¶åª’ä½“æ–‡ä»¶: {args.copy_media}")
    log(f"  å“ˆå¸Œè®¡ç®—çº¿ç¨‹æ•°: {args.hash_threads}")
    log(f"  ä½¿ç”¨å¤šè¿›ç¨‹å“ˆå¸Œ: {args.use_process}")
    log(f"  è¿›ç¨‹æ•°: {args.num_workers}")
    
    reconstruct_total_sample_nums = 0
    updated_configs = {}
    
    # è‡ªåŠ¨å‘ç°æ•°æ®é›†
    dataset_dirs = [d for d in datasets_dir.iterdir() if d.is_dir()]
    
    for dataset_dir in dataset_dirs:
        dataset_name = dataset_dir.name
        metafiles_dir = dataset_dir / args.metafiles_name
        
        if not metafiles_dir.exists():
            log(f"âš ï¸ è·³è¿‡ {dataset_name}: {args.metafiles_name}ç›®å½•ä¸å­˜åœ¨")
            continue
        
        log(f"ğŸ”„ é‡å»ºæ•°æ®é›†: {dataset_name}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        meta_save_dir = os.path.join(args.save_dir, dataset_name, args.metafiles_name)
        os.makedirs(meta_save_dir, exist_ok=True)
        
        # åªæœ‰éœ€è¦å¤åˆ¶åª’ä½“æ–‡ä»¶æ—¶æ‰åˆ›å»ºMediaFilesç›®å½•
        if args.copy_media:
            media_save_dir = os.path.join(args.save_dir, dataset_name, "MediaFiles")
            os.makedirs(media_save_dir, exist_ok=True)
        else:
            media_save_dir = None
        
        # å¤„ç†jsonlæ–‡ä»¶
        all_files = glob(os.path.join(str(metafiles_dir), "**", "*.jsonl"), recursive=True)
        
        if not all_files:
            log(f"âš ï¸ è·³è¿‡ {dataset_name}: æœªæ‰¾åˆ°jsonlæ–‡ä»¶")
            continue
        
        log(f"ğŸ“‹ {dataset_name}: æ‰¾åˆ° {len(all_files)} ä¸ªjsonlæ–‡ä»¶")
        
        # å‡†å¤‡å¤šè¿›ç¨‹ä»»åŠ¡
        jsonl_tasks = []
        for jsonl_file in all_files:
            jsonl_tasks.append([jsonl_file, dataset_name, meta_save_dir, media_save_dir])
        
        # å¤šè¿›ç¨‹å¤„ç†jsonlæ–‡ä»¶
        log(f"ğŸš€ {dataset_name}: å¼€å§‹å¤šè¿›ç¨‹å¤„ç† (ä½¿ç”¨ {min(args.num_workers, len(jsonl_tasks))} ä¸ªè¿›ç¨‹)...")
        dataset_results = process_single_jsonl_file(
            jsonl_tasks,
            num_workers=min(args.num_workers, len(jsonl_tasks))
        )
        
        # å»é‡å¹¶é‡å»ºæ•°æ®é›†
        c_sample_nums = deduplicate_and_rebuild_dataset(
            dataset_results, meta_save_dir, media_save_dir, dataset_name, args.copy_media, args.hash_threads, args.use_process
        )
        
        reconstruct_total_sample_nums += c_sample_nums
        
        updated_configs[dataset_name] = {
            args.metafiles_name: meta_save_dir,
            "sample_nums": c_sample_nums
        }
        
        log(f"âœ… å®Œæˆé‡å»º: {dataset_name} ({c_sample_nums} ä¸ªæ ·æœ¬)")
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    if args.save_yaml is not None:
        new_dataset_config = {
            "DataDir": args.save_dir,
            "Datasets": updated_configs,
            'TotalSampleNums': reconstruct_total_sample_nums
        }
        
        with open(args.save_yaml, "w", encoding='utf-8') as f:
            yaml.dump(new_dataset_config, f, default_flow_style=False, allow_unicode=True)
        
        log(f"ğŸ“‹ ä¿å­˜æ–°é…ç½®æ–‡ä»¶: {args.save_yaml}")
    
    log(f"ğŸ‰ é‡å»ºå®Œæˆ!")
    log(f"  å¤„ç†æ•°æ®é›†: {len(updated_configs)}")
    log(f"  æ€»æ ·æœ¬æ•°: {reconstruct_total_sample_nums}")
    log(f"  ä¿å­˜ç›®å½•: {args.save_dir}")
