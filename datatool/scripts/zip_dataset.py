"""
æ•°æ®é›†æ‰“åŒ…å‹ç¼©è„šæœ¬ - é«˜æ•ˆå¹¶å‘ç‰ˆæœ¬

åŠŸèƒ½æè¿°:
    è¿™ä¸ªè„šæœ¬ç”¨äºå°†åˆ†å¸ƒåœ¨ä¸åŒç›®å½•çš„æ•°æ®é›†ç»Ÿä¸€æ‰“åŒ…å‹ç¼©ï¼Œæ”¯æŒé«˜æ•ˆçš„å¤šçº¿ç¨‹å¹¶å‘å¤„ç†ã€
    åª’ä½“æ–‡ä»¶å»é‡ã€æ™ºèƒ½åˆ†å·å‹ç¼©ã€‚ç”Ÿæˆçš„å‹ç¼©åŒ…å¯ä»¥æ–¹ä¾¿åœ°åœ¨ä¸åŒæœåŠ¡å™¨é—´ä¼ è¾“å’Œéƒ¨ç½²ã€‚

ä¸»è¦åŠŸèƒ½:
    1. ğŸš€ å¤šçº¿ç¨‹å¹¶å‘å¤„ç†æ–‡ä»¶æ”¶é›†å’Œè·¯å¾„æ›´æ–° (å¤§å¹…æå‡å¤„ç†é€Ÿåº¦)
    2. ğŸ”„ æ™ºèƒ½åª’ä½“æ–‡ä»¶å»é‡ (åŸºäºæ–‡ä»¶å†…å®¹å“ˆå¸Œï¼Œé¿å…é‡å¤å­˜å‚¨)
    3. ğŸ“¦ æ™ºèƒ½åˆ†å·å‹ç¼© (é¿å…å•ä¸ªæ–‡ä»¶è¿‡å¤§)
    4. ğŸ’¾ æµå¼å‹ç¼©å¤„ç† (èŠ‚çœå†…å­˜å ç”¨)
    5. ğŸ“‹ è‡ªåŠ¨ç”Ÿæˆç»Ÿä¸€çš„é…ç½®æ–‡ä»¶
    6. ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º

å·¥ä½œæµç¨‹:
    é˜¶æ®µ1: æ”¶é›†æ‰€æœ‰æ•°æ®é¡¹ï¼ˆå•çº¿ç¨‹ï¼Œå¿«é€Ÿï¼‰
    é˜¶æ®µ2: å¤šçº¿ç¨‹å¹¶è¡Œæ”¶é›†æ‰€æœ‰åª’ä½“æ–‡ä»¶ä¿¡æ¯
    é˜¶æ®µ3: å•çº¿ç¨‹è¿›è¡Œåª’ä½“æ–‡ä»¶å»é‡ï¼ˆé¿å…ç«äº‰æ¡ä»¶ï¼‰
    é˜¶æ®µ4: å¤šçº¿ç¨‹å¹¶è¡Œæ›´æ–°æ¯ä¸ªitemçš„åª’ä½“æ–‡ä»¶è·¯å¾„
    é˜¶æ®µ5: æµå¼åˆ›å»ºzipæ–‡ä»¶ï¼Œæ”¯æŒæ™ºèƒ½åˆ†å·

åª’ä½“æ–‡ä»¶è·¯å¾„ç»“æ„ä¼˜åŒ–:
    æ—§æ ¼å¼: {dataset_name}/MediaFiles/{item_id}/{media_type}/{idx}{suffix}
    æ–°æ ¼å¼: {dataset_name}/MediaFiles/{media_type}/{1,2,3...}{suffix}
    
    ä¼˜åŠ¿: ç›¸åŒå†…å®¹çš„åª’ä½“æ–‡ä»¶åªå­˜å‚¨ä¸€æ¬¡ï¼Œä¸åŒæ ·æœ¬å¯ä»¥å…±äº«åŒä¸€ä¸ªåª’ä½“æ–‡ä»¶

è¾“å…¥æ ¼å¼:
    YAMLé…ç½®æ–‡ä»¶åŒ…å«æ•°æ®é›†ä¿¡æ¯:
    ```yaml
    DataDir: /path/to/datasets
    Datasets:
      Dataset1:
        MetaFiles: /path/to/dataset1/MetaFiles
        sample_nums: 1000
      Dataset2:
        MetaFiles: /path/to/dataset2/MetaFiles  
        sample_nums: 2000
    TotalSampleNums: 3000
    ```
ä½¿ç”¨ç¤ºä¾‹:
    # åŸºæœ¬ç”¨æ³•
    python scripts/zip_dataset.py \\
        --datasets_yaml configs/my_dataset.yaml \\
        --save_dir /tmp/exports \\
        --workers 16

ä½¿ç”¨åœºæ™¯:
    - æ•°æ®é›†è·¨æœåŠ¡å™¨è¿ç§»
    - æ•°æ®é›†ç‰ˆæœ¬ç®¡ç†å’Œå½’æ¡£
    - æ•°æ®é›†åˆ†å‘å’Œå…±äº«
    - å­˜å‚¨ç©ºé—´ä¼˜åŒ–

æ€§èƒ½ä¼˜åŒ–:
    - å¤šçº¿ç¨‹å¤„ç†: æ ¹æ®ç¡¬ä»¶é…ç½®è°ƒæ•´--workerså‚æ•°
    - åˆ†å·å¤§å°: æ ¹æ®ç½‘ç»œå’Œå­˜å‚¨ç¯å¢ƒè°ƒæ•´--max_zip_sizeå‚æ•°
    - å†…å­˜ä¼˜åŒ–: æµå¼å¤„ç†ï¼Œä¸ä¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡ä»¶åˆ°å†…å­˜

è¾“å‡ºæ–‡ä»¶:
    - å•æ–‡ä»¶: dataset_name.zip
    - åˆ†å·æ–‡ä»¶: dataset_name_part1.zip, dataset_name_part2.zip...

æ³¨æ„äº‹é¡¹:
    - ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜æ”¾å‹ç¼©æ–‡ä»¶
    - çº¿ç¨‹æ•°å»ºè®®è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°çš„1-2å€
    - åˆ†å·å¤§å°å»ºè®®æ ¹æ®ç½‘ç»œä¼ è¾“éœ€æ±‚è®¾ç½®
    - å‹ç¼©è¿‡ç¨‹ä¸­é¿å…ä¿®æ”¹æºæ–‡ä»¶
"""
import os
import json
from tqdm import tqdm
import yaml
import zipfile
from glob import glob
from pathlib import Path
import threading
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict
import io
import hashlib
from typing import Dict, List, Tuple, Any

from datatool.utils.parallel import post_allocated_multithread, post_allocated_multiprocess
from datatool.logger import log
import concurrent.futures
import time


def get_file_hash_fast(file_path):
    """å¿«é€Ÿè·å–æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œä¼˜åŒ–ç‰ˆæœ¬
    - é¦–å…ˆæ¯”è¾ƒæ–‡ä»¶å¤§å°è¿›è¡Œåˆæ­¥å»é‡
    - å¯¹äºå¤§æ–‡ä»¶ï¼Œåªè®¡ç®—æ–‡ä»¶å¤´å’Œå°¾éƒ¨çš„å“ˆå¸Œ
    - å¯¹äºå°æ–‡ä»¶ï¼Œè®¡ç®—å®Œæ•´MD5
    """
    try:
        file_path = Path(file_path)
        file_size = file_path.stat().st_size
        
        # ç©ºæ–‡ä»¶ç›´æ¥è¿”å›ç‰¹æ®Šå“ˆå¸Œ
        if file_size == 0:
            return "empty_file"
        
        # å°æ–‡ä»¶ï¼ˆ<10MBï¼‰è®¡ç®—å®Œæ•´MD5
        if file_size < 10 * 1024 * 1024:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        
        # å¤§æ–‡ä»¶åªè®¡ç®—å¤´éƒ¨ã€ä¸­é—´ã€å°¾éƒ¨çš„å“ˆå¸Œ
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            # è¯»å–æ–‡ä»¶å¤´éƒ¨ 1MB
            head_data = f.read(1024 * 1024)
            hash_md5.update(head_data)
            
            # è¯»å–æ–‡ä»¶ä¸­é—´ 1MB
            if file_size > 2 * 1024 * 1024:
                f.seek(file_size // 2)
                middle_data = f.read(1024 * 1024)
                hash_md5.update(middle_data)
            
            # è¯»å–æ–‡ä»¶å°¾éƒ¨ 1MB
            if file_size > 1024 * 1024:
                f.seek(max(0, file_size - 1024 * 1024))
                tail_data = f.read()
                hash_md5.update(tail_data)
        
        # å°†æ–‡ä»¶å¤§å°ä¹ŸåŠ å…¥å“ˆå¸Œè®¡ç®—ï¼Œå¢åŠ å”¯ä¸€æ€§
        hash_md5.update(str(file_size).encode())
        return f"fast_{hash_md5.hexdigest()}"
        
    except Exception as e:
        log(f"âš ï¸ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
        return None


def get_file_hash(file_path):
    """è·å–æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    return get_file_hash_fast(file_path)


@post_allocated_multithread
def compute_hash_for_file_zip_thread(file_info, **kwargs):
    """å¹¶å‘è®¡ç®—å•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰- zipä¸“ç”¨"""
    thread_id = kwargs.get('thread_id', 'unknown')
    
    try:
        src_path = file_info["src_path"]
        file_hash = get_file_hash(src_path)
        
        return {
            "status": "success",
            "src_path": src_path,
            "file_hash": file_hash,
            "media_file": file_info,
            "worker_id": thread_id
        }
    except Exception as e:
        return {
            "status": "error",
            "src_path": file_info.get("src_path", "unknown"),
            "error": str(e),
            "worker_id": thread_id
        }


@post_allocated_multiprocess
def compute_hash_for_file_zip_process(file_info, **kwargs):
    """å¹¶å‘è®¡ç®—å•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰- zipä¸“ç”¨"""
    process_id = kwargs.get('process_id', 'unknown')
    
    try:
        src_path = file_info["src_path"]
        file_hash = get_file_hash(src_path)
        
        return {
            "status": "success",
            "src_path": src_path,
            "file_hash": file_hash,
            "media_file": file_info,
            "worker_id": process_id
        }
    except Exception as e:
        return {
            "status": "error",
            "src_path": file_info.get("src_path", "unknown"),
            "error": str(e),
            "worker_id": process_id
        }


def collect_all_data_items(yaml_path):
    """æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„æ•°æ®é¡¹ï¼ˆä¸å»é‡ï¼‰"""
    with open(yaml_path, "r") as f:
        dataset_config = yaml.safe_load(f)
    
    all_items = []
    
    # éå†æ‰€æœ‰ Datasets
    for dataset_name, config in dataset_config["Datasets"].items():
        metafile_dir = config["MetaFiles"]
        
        # é€’å½’æ‰¾åˆ°æ‰€æœ‰ .jsonl æ–‡ä»¶
        all_metafiles = glob(os.path.join(metafile_dir, "**", "*.jsonl"), recursive=True)
        
        for metafile in all_metafiles:
            with open(metafile, "r") as f:
                lines = f.readlines()
                for line_idx, line in enumerate(lines):
                    if not line.strip():
                        continue
                    try:
                        meta_item = json.loads(line)
                        all_items.append({
                            "dataset_name": dataset_name,
                            "metafile": metafile,
                            "line_idx": line_idx,
                            "meta_item": meta_item
                        })
                    except json.JSONDecodeError as e:
                        log(f"JSON decode error in {metafile}:{line_idx}: {e}")
    
    return all_items, dataset_config


@post_allocated_multithread
def collect_media_files_from_item(item_data: Dict[str, Any], **kwargs) -> List[Dict[str, Any]]:
    """ä»å•ä¸ªitemä¸­æ”¶é›†æ‰€æœ‰åª’ä½“æ–‡ä»¶ä¿¡æ¯ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰"""
    dataset_name = item_data["dataset_name"]
    meta_item = item_data["meta_item"]
    item_id = meta_item["id"]
    
    media_files = []
    
    # å¤„ç† images/videos/audios
    for media_type in ["images", "videos", "audios"]:
        media_paths = meta_item.get(media_type, [])
        if not media_paths:
            continue
        
        for idx, src_path in enumerate(media_paths):
            src_path = Path(src_path)
            if not src_path.exists():
                log(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
                continue
                
            try:
                file_size = src_path.stat().st_size
                suffix = src_path.suffix if src_path.suffix else ""
                
                media_files.append({
                    "src_path": str(src_path),
                    "media_type": media_type,
                    "suffix": suffix,
                    "size": file_size,
                    "dataset_name": dataset_name,
                    "item_id": item_id,
                    "item_idx": idx  # åœ¨itemå†…çš„ç´¢å¼•
                })
            except Exception as e:
                log(f"âš ï¸ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {src_path}: {e}")
                continue
        
    return media_files


def deduplicate_media_files_byhash(all_media_files: List[Dict[str, Any]], hash_threads: int = 16, use_process: bool = False) -> Tuple[Dict[str, Dict], Dict[str, str]]:
    """å¯¹åª’ä½“æ–‡ä»¶è¿›è¡Œå»é‡ï¼Œè¿”å›å”¯ä¸€æ–‡ä»¶æ˜ å°„å’Œè·¯å¾„æ˜ å°„ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸¤é˜¶æ®µå»é‡ + å¹¶å‘å“ˆå¸Œï¼‰
    
    Returns:
        unique_files: {file_hash: {media_type, src_path, idx, suffix, size}}
        path_mapping: {original_src_path: new_relative_path}
    """
    log("ğŸ” å¼€å§‹åª’ä½“æ–‡ä»¶å»é‡...")
    
    unique_files = {}  # {file_hash: file_info}
    path_mapping = {}  # {src_path: relative_path}
    media_counters = defaultdict(lambda: defaultdict(int))  # {dataset: {media_type: counter}} - æŒ‰æ•°æ®é›†åˆ†åˆ«è®¡æ•°
    size_groups = defaultdict(list)  # {file_size: [media_files]} - æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
    
    # ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„ï¼Œå¿«é€Ÿè¿‡æ»¤
    log("ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„...")
    processed_paths = set()  # ç”¨äºè·Ÿè¸ªå·²å¤„ç†çš„è·¯å¾„
    for media_file in tqdm(all_media_files, desc="åˆ†ç»„åª’ä½“æ–‡ä»¶"):
        src_path = media_file["src_path"]
        
        # å¦‚æœå·²ç»å¤„ç†è¿‡è¿™ä¸ªè·¯å¾„ï¼Œè·³è¿‡
        if src_path in processed_paths:
            continue
        processed_paths.add(src_path)
        
        try:
            file_size = Path(src_path).stat().st_size
            media_file["file_size"] = file_size
            size_groups[file_size].append(media_file)
        except Exception as e:
            log(f"âš ï¸ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {src_path}: {e}")
            continue
    
    log(f"ğŸ“Š å¤§å°åˆ†ç»„ç»Ÿè®¡ - {len(size_groups)} ä¸ªä¸åŒå¤§å°çš„ç»„")
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¯¹æ¯ä¸ªå¤§å°ç»„å†…çš„æ–‡ä»¶è®¡ç®—å“ˆå¸Œ
    log("ğŸ” ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—å“ˆå¸Œå»é‡...")
    for file_size, media_files_group in tqdm(size_groups.items(), desc="å¤„ç†å¤§å°ç»„"):
        if len(media_files_group) == 1:
            # è¯¥å¤§å°åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œæ— éœ€è®¡ç®—å“ˆå¸Œï¼Œç›´æ¥å¤„ç†
            media_file = media_files_group[0]
            src_path = media_file["src_path"]
            
            media_type = media_file["media_type"]
            dataset_name = media_file["dataset_name"]
            media_counters[dataset_name][media_type] += 1
            idx = media_counters[dataset_name][media_type]
            suffix = media_file["suffix"]
            
            # ä½¿ç”¨æ–‡ä»¶å¤§å°ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆæ— éœ€è®¡ç®—å“ˆå¸Œï¼‰
            size_hash = f"size_{file_size}_{src_path}"
            unique_files[size_hash] = {
                "media_type": media_type,
                "src_path": src_path,
                "idx": idx,
                "suffix": suffix,
                "size": media_file["size"],
                "original_dataset": dataset_name
            }
            
            rel_path = f"MediaFiles/{media_type}/{idx}{suffix}"
            path_mapping[src_path] = rel_path
        
        else:
            # è¯¥å¤§å°æœ‰å¤šä¸ªæ–‡ä»¶ï¼Œéœ€è¦è®¡ç®—å“ˆå¸Œå»é‡ï¼ˆä½¿ç”¨å¹¶å‘ï¼‰
            process_hash_group_concurrent_zip(
                media_files_group, 
                media_counters, 
                unique_files, 
                path_mapping,
                max_workers=hash_threads,
                use_process=use_process
            )
    
    log(f"ğŸ“Š åª’ä½“æ–‡ä»¶å»é‡ç»Ÿè®¡:")
    log(f"   åŸå§‹æ–‡ä»¶æ•°: {len(all_media_files)}")
    log(f"   å»é‡åæ–‡ä»¶æ•°: {len(unique_files)}")
    log(f"   èŠ‚çœæ–‡ä»¶æ•°: {len(all_media_files) - len(unique_files)}")
    
    # æŒ‰æ•°æ®é›†ç»Ÿè®¡
    for dataset_name, dataset_counters in media_counters.items():
        total_dataset_files = sum(dataset_counters.values())
        log(f"   {dataset_name}: {total_dataset_files} ä¸ªå”¯ä¸€æ–‡ä»¶")
        for media_type, count in dataset_counters.items():
            log(f"     {media_type}: {count} ä¸ª")
    
    return unique_files, path_mapping

def deduplicate_media_files_by_path(all_media_files: List[Dict[str, Any]]) -> Tuple[Dict[str, Dict], Dict[str, str]]:
    """
    ç®€åŒ–ç‰ˆï¼šä»…åŸºäºæ–‡ä»¶è·¯å¾„å»é‡ã€‚
    
    Returns:
        unique_files: {src_path: {media_type, src_path, idx, suffix, size, original_dataset}}
        path_mapping: {src_path: relative_path}
    """
    log("ğŸ” å¼€å§‹åŸºäºè·¯å¾„çš„åª’ä½“æ–‡ä»¶å»é‡...")
    
    unique_files = {}
    path_mapping = {}
    media_counters = defaultdict(lambda: defaultdict(int))
    processed_paths = set()
    
    for media_file in tqdm(all_media_files, desc="æ£€æŸ¥åª’ä½“æ–‡ä»¶è·¯å¾„"):
        src_path = media_file["src_path"]
        if src_path in processed_paths:
            continue  # å·²å‡ºç°è¿‡çš„è·¯å¾„ç›´æ¥è·³è¿‡
        
        processed_paths.add(src_path)
        
        media_type = media_file["media_type"]
        dataset_name = media_file["dataset_name"]
        suffix = media_file["suffix"]
        
        media_counters[dataset_name][media_type] += 1
        idx = media_counters[dataset_name][media_type]

        # ç”Ÿæˆä¿å­˜è·¯å¾„
        rel_path = f"MediaFiles/{media_type}/{idx}{suffix}"
        
        unique_files[src_path] = {
            "media_type": media_type,
            "src_path": src_path,
            "idx": idx,
            "suffix": suffix,
            "size": media_file.get("size"),
            "original_dataset": dataset_name,
        }
        path_mapping[src_path] = rel_path
    
    log(f"ğŸ“Š è·¯å¾„å»é‡ç»Ÿè®¡: åŸå§‹æ–‡ä»¶ {len(all_media_files)} â†’ å”¯ä¸€æ–‡ä»¶ {len(unique_files)}")
    return unique_files, path_mapping


def process_hash_group_concurrent_zip(media_files_group, media_counters, unique_files, path_mapping, max_workers=16, use_process=False):
    """å¹¶å‘å¤„ç†éœ€è¦è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶ç»„ï¼ˆzipä¸“ç”¨ï¼‰"""
    
    # å‡†å¤‡å“ˆå¸Œè®¡ç®—ä»»åŠ¡
    hash_tasks = []
    for media_file in media_files_group:
        src_path = media_file["src_path"]
        if Path(src_path).exists():
            hash_tasks.append(media_file)
    
    if not hash_tasks:
        return
    
    worker_type = "è¿›ç¨‹" if use_process else "çº¿ç¨‹"
    log(f"ğŸ”§ å¹¶å‘è®¡ç®— {len(hash_tasks)} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ (ä½¿ç”¨ {min(max_workers, len(hash_tasks))} ä¸ª{worker_type})...")
    
    # é€‰æ‹©å¹¶å‘è®¡ç®—æ–¹å¼
    if use_process:
        # CPUå¯†é›†å‹ï¼šä½¿ç”¨å¤šè¿›ç¨‹
        hash_results = compute_hash_for_file_zip_process(
            hash_tasks,
            num_workers=min(max_workers, len(hash_tasks))
        )
    else:
        # I/Oå¯†é›†å‹ï¼šä½¿ç”¨å¤šçº¿ç¨‹
        hash_results = compute_hash_for_file_zip_thread(
            hash_tasks,
            num_workers=min(max_workers, len(hash_tasks))
        )
    
    # å¤„ç†å“ˆå¸Œç»“æœï¼Œè¿›è¡Œå»é‡
    group_hash_map = {}
    for result in hash_results:
        if result["status"] != "success" or not result["file_hash"]:
            continue
        
        file_hash = result["file_hash"]
        media_file = result["media_file"]
        src_path = media_file["src_path"]
        
        if file_hash not in group_hash_map:
            # æ–°çš„å”¯ä¸€æ–‡ä»¶
            media_type = media_file["media_type"]
            dataset_name = media_file["dataset_name"]
            media_counters[dataset_name][media_type] += 1
            idx = media_counters[dataset_name][media_type]
            suffix = media_file["suffix"]
            
            group_hash_map[file_hash] = {
                "media_type": media_type,
                "src_path": src_path,
                "idx": idx,
                "suffix": suffix,
                "size": media_file["size"],
                "original_dataset": dataset_name
            }
            
            unique_files[file_hash] = group_hash_map[file_hash]
            
            rel_path = f"MediaFiles/{media_type}/{idx}{suffix}"
            path_mapping[src_path] = rel_path
        else:
            # é‡å¤æ–‡ä»¶ï¼Œä½¿ç”¨å·²æœ‰çš„è·¯å¾„æ˜ å°„
            existing_info = group_hash_map[file_hash]
            rel_path = f"MediaFiles/{existing_info['media_type']}/{existing_info['idx']}{existing_info['suffix']}"
            path_mapping[src_path] = rel_path


@post_allocated_multithread
def update_item_paths(item_data: Dict[str, Any], path_mapping: Dict[str, str], **kwargs) -> Dict[str, Any]:
    """æ›´æ–°å•ä¸ªitemä¸­çš„åª’ä½“æ–‡ä»¶è·¯å¾„ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰"""
    dataset_name = item_data["dataset_name"]
    meta_item = item_data["meta_item"].copy()
    
    # æ›´æ–°åª’ä½“æ–‡ä»¶è·¯å¾„
    for media_type in ["images", "videos", "audios"]:
        media_paths = meta_item.get(media_type, [])
        if not media_paths:
            continue
        
        new_paths = []
        for src_path in media_paths:
            # æ ‡å‡†åŒ–è·¯å¾„
            src_path = str(Path(src_path))
            if src_path in path_mapping:
                new_paths.append(path_mapping[src_path])
            else:
                log(f"âš ï¸ æ‰¾ä¸åˆ°è·¯å¾„æ˜ å°„: {src_path}")
        
        meta_item[media_type] = new_paths
    
    return {
        "dataset_name": dataset_name,
        "metafile": item_data["metafile"],
        "updated_meta_item": meta_item
    }


def create_single_zip_concurrent(zip_info):
    """å¹¶å‘åˆ›å»ºå•ä¸ªzipæ–‡ä»¶ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰"""
    zip_path, files_to_add, jsonl_content, config_content = zip_info
    
    try:
        start_time = time.time()
        log(f"ğŸ”§ å¼€å§‹åˆ›å»º: {Path(zip_path).name} (åŒ…å« {len(files_to_add)} ä¸ªæ–‡ä»¶)")
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        Path(zip_path).parent.mkdir(parents=True, exist_ok=True)
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=1, allowZip64=True) as zipf:
            # æ·»åŠ é…ç½®æ–‡ä»¶
            if config_content:
                zipf.writestr("dataset_config.yaml", config_content)
            
            # æ·»åŠ jsonlæ–‡ä»¶
            for jsonl_path, content in jsonl_content.items():
                zipf.writestr(jsonl_path, content.encode('utf-8'))
            
            # æ‰¹é‡æ·»åŠ åª’ä½“æ–‡ä»¶ï¼ˆä¼˜åŒ–I/Oï¼‰
            for i, file_info in enumerate(files_to_add):
                src_path = file_info["src_path"]
                zip_path_in_archive = file_info["zip_path"]
                
                try:
                    # ç›´æ¥å†™å…¥ï¼Œå‡å°‘ä¸­é—´ç¼“å­˜
                    zipf.write(src_path, zip_path_in_archive)
                    
                    # æ¯100ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if (i + 1) % 100 == 0:
                        log(f"ğŸ”§ {Path(zip_path).name}: å·²å¤„ç† {i+1}/{len(files_to_add)} ä¸ªæ–‡ä»¶")
                except Exception as e:
                    log(f"âš ï¸ æ·»åŠ æ–‡ä»¶å¤±è´¥ {src_path}: {e}")
                    continue
        
        elapsed = time.time() - start_time
        log(f"âœ… å®Œæˆåˆ›å»º: {Path(zip_path).name} ({elapsed:.2f}s)")
        return {"status": "success", "zip_path": zip_path, "time": elapsed}
        
    except Exception as e:
        log(f"âŒ åˆ›å»ºzipå¤±è´¥ {zip_path}: {e}")
        return {"status": "error", "zip_path": zip_path, "error": str(e)}


def allocate_files_to_volumes(unique_file_paths, jsonl_data, config_content, max_zip_size):
    """æ™ºèƒ½åˆ†é…æ–‡ä»¶åˆ°ä¸åŒå·"""
    volumes = []
    current_volume = {
        "files": [],
        "jsonl": jsonl_data,  # æ¯ä¸ªå·éƒ½åŒ…å«å®Œæ•´çš„JSONLæ•°æ®
        "config": config_content,  # åªæœ‰ç¬¬ä¸€ä¸ªå·åŒ…å«é…ç½®
        "size": 0
    }
    
    # è®¡ç®—åŸºç¡€å¼€é”€ï¼ˆJSONL + configï¼‰
    base_overhead = sum(len(content.encode('utf-8')) for content in jsonl_data.values())
    if config_content:
        base_overhead += len(config_content.encode('utf-8'))
    
    current_volume["size"] = base_overhead
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œå¤§æ–‡ä»¶ä¼˜å…ˆ
    sorted_files = sorted(unique_file_paths, key=lambda x: x["size"], reverse=True)
    
    for file_info in sorted_files:
        file_size = file_info["size"]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°å·
        if current_volume["size"] + file_size > max_zip_size and current_volume["files"]:
            # å½“å‰å·å·²æ»¡ï¼Œåˆ›å»ºæ–°å·
            volumes.append(current_volume)
            current_volume = {
                "files": [],
                "jsonl": jsonl_data,
                "config": None,  # åªæœ‰ç¬¬ä¸€ä¸ªå·æœ‰é…ç½®
                "size": base_overhead
            }
        
        current_volume["files"].append(file_info)
        current_volume["size"] += file_size
    
    # æ·»åŠ æœ€åä¸€ä¸ªå·
    if current_volume["files"]:
        volumes.append(current_volume)
    
    return volumes


def create_zip_files_streaming_optimized(updated_results, unique_files, output_path, max_zip_size_mb=2048, num_workers=4):
    """ä¼˜åŒ–çš„æµå¼åˆ›å»ºzipæ–‡ä»¶ï¼Œæ”¯æŒåˆ†å·å’Œå¹¶å‘å‹ç¼©"""
    max_zip_size = max_zip_size_mb * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
    
    # é‡æ–°ç»„ç»‡æ•°æ®ï¼ŒæŒ‰åŸå§‹jsonlæ–‡ä»¶åˆ†ç»„
    file_results = defaultdict(list)  # {(dataset_name, metafile_path): [items]}
    dataset_results = defaultdict(list)  # {dataset_name: [items]} - ä¿æŒå…¼å®¹æ€§
    
    for result in updated_results:
        dataset_name = result["dataset_name"]
        metafile_path = result["metafile"]
        updated_item = result["updated_meta_item"]
        
        # æŒ‰åŸå§‹æ–‡ä»¶åˆ†ç»„
        file_key = (dataset_name, metafile_path)
        file_results[file_key].append(updated_item)
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„ï¼ˆç”¨äºé…ç½®æ–‡ä»¶ï¼‰
        dataset_results[dataset_name].append(updated_item)
    
    # åˆ›å»ºdataset_config.yamlå†…å®¹
    updated_datasets = {}
    for dataset_name, items in dataset_results.items():
        updated_datasets[dataset_name] = {
            "MetaFiles": f"{dataset_name}/MetaFiles",
            "sample_nums": len(items)
        }
    
    updated_config = {
        "DataDir": None,
        "Datasets": updated_datasets,
        "TotalSampleNums": sum(len(items) for items in dataset_results.values())
    }
    
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
    unique_file_paths = []
    dataset_file_counters = defaultdict(lambda: defaultdict(int))  # {dataset: {media_type: count}}
    
    # ç›´æ¥ä»unique_filesç”Ÿæˆæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    for file_hash, info in unique_files.items():
        src_path = info['src_path']
        media_type = info['media_type']
        suffix = info['suffix']
        original_dataset = info['original_dataset']
        
        # ä¸ºåŸå§‹æ•°æ®é›†ç”Ÿæˆæ–‡ä»¶è·¯å¾„
        dataset_file_counters[original_dataset][media_type] += 1
        idx = dataset_file_counters[original_dataset][media_type]
        
        zip_path = f"{original_dataset}/MediaFiles/{media_type}/{idx}{suffix}"
        unique_file_paths.append({
            "src_path": src_path,
            "zip_path": zip_path,
            "size": info['size'],
            "file_hash": file_hash,
            "dataset": original_dataset
        })
    
    # è®¡ç®—æ€»å¤§å°
    total_size = sum(fp["size"] for fp in unique_file_paths)
    # åŠ ä¸Šé…ç½®æ–‡ä»¶å¤§å°ï¼ˆä¼°ç®—ï¼‰
    config_yaml = yaml.dump(updated_config, default_flow_style=False, allow_unicode=True)
    total_size += len(config_yaml.encode('utf-8'))
    
    # åŠ ä¸Šjsonlæ–‡ä»¶å¤§å°ï¼ˆä¼°ç®—ï¼‰
    for dataset_name, items in dataset_results.items():
        jsonl_size = sum(len(json.dumps(item, ensure_ascii=False)) + 1 for item in items)
        total_size += jsonl_size
    
    total_size_mb = total_size / (1024 * 1024)
    log(f"ğŸ“Š æ€»æ•°æ®å¤§å°: {total_size_mb:.1f} MB (å»é‡å)")
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œå¤§æ–‡ä»¶ä¼˜å…ˆï¼ˆæ›´å¥½çš„åˆ†å·å¹³è¡¡ï¼‰
    unique_file_paths.sort(key=lambda x: x["size"], reverse=True)
    
    if total_size <= max_zip_size:
        # å•ä¸ªzipæ–‡ä»¶ - ä½¿ç”¨ä¼˜åŒ–çš„å¹¶å‘å‹ç¼©é€»è¾‘
        zip_path = f"{output_path}.zip"
        log(f"ğŸ“¦ åˆ›å»ºå•ä¸ªzipæ–‡ä»¶: {zip_path}")
        
        # å‡†å¤‡JSONLæ•°æ®ï¼ˆä¸åˆ†å·ç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰
        jsonl_data = {}
        for (dataset_name, metafile_path), items in file_results.items():
            original_filename = Path(metafile_path).name
            jsonl_path = f"{dataset_name}/MetaFiles/{original_filename}"
            
            jsonl_content = ""
            for item in items:
                jsonl_content += json.dumps(item, ensure_ascii=False) + "\n"
            
            jsonl_data[jsonl_path] = jsonl_content
        
        # ä½¿ç”¨ç»Ÿä¸€çš„å¹¶å‘å‹ç¼©å‡½æ•°
        zip_info = (zip_path, unique_file_paths, jsonl_data, config_yaml)
        result = create_single_zip_concurrent(zip_info)
        
        if result["status"] == "success":
            return [zip_path]
        else:
            log(f"âŒ å‹ç¼©å¤±è´¥: {result.get('error', 'Unknown error')}")
            return []
    
    else:
        # åˆ†å·å‹ç¼© - æ™ºèƒ½åˆ†é… + å¹¶å‘å‹ç¼©
        log(f"ğŸ“¦ æ•°æ®è¿‡å¤§ï¼Œå¼€å§‹æ™ºèƒ½åˆ†å·å‹ç¼©...")
        
        # å‡†å¤‡JSONLæ•°æ®ï¼ˆä¸å•æ–‡ä»¶ç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰
        jsonl_data = {}
        for (dataset_name, metafile_path), items in file_results.items():
            original_filename = Path(metafile_path).name
            jsonl_path = f"{dataset_name}/MetaFiles/{original_filename}"
            
            jsonl_content = ""
            for item in items:
                jsonl_content += json.dumps(item, ensure_ascii=False) + "\n"
            
            jsonl_data[jsonl_path] = jsonl_content
        
        volumes = allocate_files_to_volumes(unique_file_paths, jsonl_data, config_yaml, max_zip_size)
        log(f"ğŸ“¦ æ™ºèƒ½åˆ†é…ä¸º {len(volumes)} ä¸ªåˆ†å·")
        
        # å‡†å¤‡å¹¶å‘å‹ç¼©ä»»åŠ¡
        zip_tasks = []
        for vol_idx, volume in enumerate(volumes):
            zip_path = f"{output_path}_part{vol_idx + 1}.zip"
            zip_info = (zip_path, volume["files"], volume["jsonl"], volume["config"])
            zip_tasks.append(zip_info)
        
        # å¹¶å‘å‹ç¼©æ‰€æœ‰åˆ†å·
        log(f"ğŸš€ å¹¶å‘å‹ç¼© {len(zip_tasks)} ä¸ªåˆ†å· (ä½¿ç”¨ {min(num_workers, len(zip_tasks))} ä¸ªçº¿ç¨‹)...")
        
        zip_paths = []
        total_time = 0
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(num_workers, len(zip_tasks))) as executor:
            # æäº¤æ‰€æœ‰å‹ç¼©ä»»åŠ¡
            future_to_path = {
                executor.submit(create_single_zip_concurrent, zip_info): zip_info[0] 
                for zip_info in zip_tasks
            }
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_path):
                zip_path = future_to_path[future]
                try:
                    result = future.result()
                    if result["status"] == "success":
                        zip_paths.append(result["zip_path"])
                        total_time += result["time"]
                    else:
                        log(f"âŒ åˆ†å·å‹ç¼©å¤±è´¥ {zip_path}: {result.get('error', 'Unknown error')}")
                except Exception as e:
                    log(f"âŒ å‹ç¼©ä»»åŠ¡å¼‚å¸¸ {zip_path}: {e}")
            
        # æŒ‰åºæ’åˆ—ç»“æœ
        zip_paths.sort()
        
        log(f"âœ… æ‰€æœ‰åˆ†å·å‹ç¼©å®Œæˆ (æ€»è€—æ—¶: {total_time:.2f}s)")
        return zip_paths


def process_datasets(yaml_path, save_dir, max_zip_size_mb=2048, num_workers=16, use_process=False):
    """ä¸»å¤„ç†å‡½æ•° - åˆ†é˜¶æ®µå¹¶å‘å®‰å…¨å¤„ç†"""
    yaml_path = Path(yaml_path)
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ä»yamlæ–‡ä»¶åæ¨æ–­è¾“å‡ºæ–‡ä»¶å
    output_name = yaml_path.stem
    output_path = save_dir / output_name
    
    # é˜¶æ®µ1: æ”¶é›†æ‰€æœ‰æ•°æ®é¡¹
    log("ğŸ“‹ é˜¶æ®µ1: æ”¶é›†æ•°æ®é¡¹...")
    all_items, dataset_config = collect_all_data_items(yaml_path)
    log(f"ğŸ“‹ å…±æ‰¾åˆ° {len(all_items)} ä¸ªæ•°æ®é¡¹")
    
    if not all_items:
        log("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®é¡¹")
        return
    
    # é˜¶æ®µ2: å¹¶è¡Œæ”¶é›†æ‰€æœ‰åª’ä½“æ–‡ä»¶ä¿¡æ¯ï¼ˆå¤šçº¿ç¨‹ï¼‰
    log(f"ğŸš€ é˜¶æ®µ2: å¹¶è¡Œæ”¶é›†åª’ä½“æ–‡ä»¶ä¿¡æ¯ (ä½¿ç”¨ {num_workers} ä¸ªçº¿ç¨‹)...")
    all_media_files_nested = collect_media_files_from_item(all_items, num_workers=num_workers)
    
    # å±•å¹³åµŒå¥—åˆ—è¡¨
    all_media_files = []
    for media_list in all_media_files_nested:
        if isinstance(media_list, list):
            all_media_files.extend(media_list)
        else:
            all_media_files.append(media_list)
    
    log(f"ğŸ“Š æ”¶é›†åˆ° {len(all_media_files)} ä¸ªåª’ä½“æ–‡ä»¶")
        
    # é˜¶æ®µ3: å¹¶å‘è¿›è¡Œåª’ä½“æ–‡ä»¶å»é‡
    if args.need_deduplicate:    
        worker_type = "å¤šè¿›ç¨‹" if use_process else "å¤šçº¿ç¨‹"
        log(f"ğŸ”„ é˜¶æ®µ3: åª’ä½“æ–‡ä»¶å»é‡ï¼ˆ{worker_type}ï¼‰...")
        unique_files, path_mapping = deduplicate_media_files_by_path(all_media_files)
    
        #é˜¶æ®µ4ï¼šå¹¶è¡Œæ›´æ–°æ¯ä¸ªitemçš„è·¯å¾„ï¼ˆå¤šçº¿ç¨‹ï¼‰
        log(f"ğŸ“ é˜¶æ®µ4: å¹¶è¡Œæ›´æ–°itemè·¯å¾„ (ä½¿ç”¨ {num_workers} ä¸ªçº¿ç¨‹)...")
        updated_results = update_item_paths(all_items, path_mapping=path_mapping, num_workers=num_workers)
    else:
        updated_results = all_items
        unique_files = all_media_files
    
    # é˜¶æ®µ5: æµå¼åˆ›å»ºzipæ–‡ä»¶
    log("ğŸ“¦ é˜¶æ®µ5: å¼€å§‹æµå¼å‹ç¼©...")
    zip_files = create_zip_files_streaming_optimized(updated_results, unique_files, str(output_path), max_zip_size_mb, num_workers)
    
    log(f"ğŸ‰ å®Œæˆï¼å…±å¤„ç† {len(updated_results)} ä¸ªæ•°æ®é¡¹")
    log(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {', '.join(zip_files)}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤„ç†æ•°æ®é›†å¹¶ç›´æ¥å‹ç¼©ï¼ˆæ”¯æŒåˆ†å·ï¼‰")
    parser.add_argument("--datasets_yaml",default="/home/gary/AI-Project/yjy/test/datatooltest/test.yaml", help="è¾“å…¥çš„yamlé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--save_dir",default="/home/gary/AI-Project/yjy/test/datatooltest/zip/", help="è¾“å‡ºç›®å½•")
    parser.add_argument('--max_zip_size', type=int, default=1024, help="å•ä¸ªzipæ–‡ä»¶æœ€å¤§å¤§å°(MB)ï¼Œè¶…è¿‡ä¼šè‡ªåŠ¨åˆ†å·")
    parser.add_argument('--num_workers', type=int, default=16, help="å¹¶å‘çº¿ç¨‹æ•°")
    parser.add_argument('--use_process', action='store_true', help="ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œå“ˆå¸Œè®¡ç®—ï¼ˆCPUå¯†é›†å‹ä¼˜åŒ–ï¼‰")
    parser.add_argument('--need_deduplicate', type=bool, default= True, help="æ˜¯å¦è¿›è¡Œé‡å¤mediaæ–‡ä»¶å»é‡")
    
    args = parser.parse_args()
    process_datasets(args.datasets_yaml, args.save_dir, args.max_zip_size, args.num_workers, args.use_process)
