#!/usr/bin/env python3
"""
æ•°æ®é›†è§£å‹å’Œè·¯å¾„é‡æ„è„šæœ¬ - é«˜æ•ˆä¼˜åŒ–ç‰ˆæœ¬

åŠŸèƒ½æè¿°:
    è¿™ä¸ªè„šæœ¬ç”¨äºè§£å‹ç”± zip_dataset.py ç”Ÿæˆçš„æ•°æ®é›†å‹ç¼©åŒ…ï¼Œå¹¶å°†å…¶ä¸­çš„ç›¸å¯¹è·¯å¾„
    è½¬æ¢ä¸ºç›®æ ‡æœåŠ¡å™¨ä¸Šçš„ç»å¯¹è·¯å¾„ã€‚æ”¯æŒå•ä¸ªzipæ–‡ä»¶å’Œåˆ†å·zipæ–‡ä»¶çš„è§£å‹ã€‚

ä¸»è¦åŠŸèƒ½:
    1. ğŸš€ å¤šè¿›ç¨‹å¹¶å‘è§£å‹zipæ–‡ä»¶ (å……åˆ†åˆ©ç”¨å¤šCPUï¼Œå¤§å¹…æå‡è§£å‹é€Ÿåº¦)
    2. ğŸ“¦ æ™ºèƒ½è¯†åˆ«å’Œå¤„ç†zipæ–‡ä»¶ (è‡ªåŠ¨åŒ¹é…*.zip, *_part*.zipç­‰æ¨¡å¼)
    3. ğŸ”„ é«˜æ•ˆæ‰¹é‡æ›´æ–°jsonlæ–‡ä»¶ä¸­çš„åª’ä½“æ–‡ä»¶è·¯å¾„ (images/videos/audios)
    4. ğŸ“‹ æ›´æ–°yamlé…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®é›†è·¯å¾„
    5. ğŸ›¡ï¸ å®Œå–„çš„é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º

ä½¿ç”¨åœºæ™¯:
    - æ•°æ®é›†åœ¨ä¸åŒæœåŠ¡å™¨é—´è¿ç§»
    - ä»å‹ç¼©åŒ…æ¢å¤æ•°æ®é›†åˆ°æ–°çš„å­˜å‚¨ä½ç½®
    - æ‰¹é‡æ›´æ–°æ•°æ®é›†è·¯å¾„é…ç½®

ç›®å½•ç»“æ„:
    è§£å‹å‰çš„zipå†…å®¹:
    â”œâ”€â”€ dataset_config.yaml
    â”œâ”€â”€ Dataset1/
    â”‚   â”œâ”€â”€ MetaFiles/Dataset1.jsonl
    â”‚   â””â”€â”€ MediaFiles/item1/images/0.jpg
    
    è§£å‹åæ›´æ–°è·¯å¾„:
    â”œâ”€â”€ dataset_config.yaml (DataDiræ›´æ–°ä¸ºnew_base_path)
    â””â”€â”€ Dataset1/
        â”œâ”€â”€ MetaFiles/Dataset1.jsonl (è·¯å¾„æ›´æ–°ä¸ºç»å¯¹è·¯å¾„)
        â””â”€â”€ MediaFiles/... (æ–‡ä»¶ä½ç½®ä¸å˜)


æ³¨æ„äº‹é¡¹:
    - extract_dir å’Œ base_path é€šå¸¸è®¾ç½®ä¸ºç›¸åŒå€¼
    - ç¡®ä¿ç›®æ ‡ç›®å½•æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
    - å¤šè¿›ç¨‹æ•°é‡å»ºè®®è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°çš„1-2å€
"""

import os
import json
import yaml
import argparse
import zipfile
from pathlib import Path
from glob import glob
from tqdm import tqdm
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
from typing import List, Dict, Any, Tuple

from datatool.utils.parallel import post_allocated_multiprocess
from datatool.logger import log


@post_allocated_multiprocess
def extract_zip_parallel(zip_file_info: Tuple[str, str], **kwargs) -> Dict[str, Any]:
    """ä½¿ç”¨post_allocated_multiprocesså¹¶è¡Œè§£å‹å•ä¸ªzipæ–‡ä»¶"""
    return extract_single_zip(zip_file_info, **kwargs)


def extract_single_zip(zip_file_info: Tuple[str, str], **kwargs) -> Dict[str, Any]:
    """è§£å‹å•ä¸ªzipæ–‡ä»¶ï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰"""
    zip_file, extract_dir = zip_file_info
    process_id = kwargs.get("process_id", "unknown")
    
    try:
        if not Path(zip_file).exists():
            return {"status": "error", "file": zip_file, "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
        
        log(f"ğŸ”§ è¿›ç¨‹ {process_id} å¼€å§‹è§£å‹: {Path(zip_file).name}")
        start_time = time.time()
        
        with zipfile.ZipFile(zip_file, 'r') as zipf:
            # è·å–æ–‡ä»¶ä¿¡æ¯ç”¨äºè¿›åº¦æ˜¾ç¤º
            file_count = len(zipf.namelist())
            
            # å®‰å…¨è§£å‹æ‰€æœ‰æ–‡ä»¶ï¼ˆå¤„ç†å¹¶å‘ç›®å½•åˆ›å»ºå†²çªï¼‰
            try:
                zipf.extractall(extract_dir)
            except FileExistsError as e:
                # å¤„ç†å¹¶å‘ç›®å½•åˆ›å»ºå†²çªï¼Œé€ä¸ªæ–‡ä»¶è§£å‹
                log(f"ğŸ”„ è¿›ç¨‹ {process_id} æ£€æµ‹åˆ°ç›®å½•å†²çªï¼Œåˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼è§£å‹")
                for member in zipf.namelist():
                    try:
                        zipf.extract(member, extract_dir)
                    except FileExistsError:
                        # å¦‚æœæ˜¯ç›®å½•å†²çªï¼Œå¿½ç•¥ï¼›å¦‚æœæ˜¯æ–‡ä»¶å†²çªï¼Œè·³è¿‡
                        if not member.endswith('/'):
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å†…å®¹ç›¸åŒ
                            target_file = Path(extract_dir) / member
                            if target_file.exists():
                                continue  # æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
                        continue
        
        duration = time.time() - start_time
        
        return {
            "status": "success", 
            "file": zip_file, 
            "file_count": file_count,
            "duration": duration,
            "size_mb": Path(zip_file).stat().st_size / (1024 * 1024),
            "process_id": process_id
        }
        
    except Exception as e:
        return {"status": "error", "file": zip_file, "message": str(e), "process_id": process_id}


def extract_zip_files(zip_pattern: str, zip_source_dir: str, extract_dir: str, num_workers: int = 8) -> Path:
    """æ™ºèƒ½å¤šè¿›ç¨‹è§£å‹æ‰€æœ‰zipæ–‡ä»¶"""
    zip_source_dir = Path(zip_source_dir)
    extract_dir = Path(extract_dir)
    extract_dir.mkdir(parents=True, exist_ok=True)
    
    # æ™ºèƒ½æŸ¥æ‰¾zipæ–‡ä»¶
    def find_zip_files():
        if "*" in zip_pattern:
            # é€šé…ç¬¦æ¨¡å¼
            if not os.path.isabs(zip_pattern):
                search_pattern = zip_source_dir / zip_pattern
            else:
                search_pattern = zip_pattern
            files = glob(str(search_pattern))
        else:
            # ç›´æ¥æŒ‡å®šæ–‡ä»¶
            if not os.path.isabs(zip_pattern):
                zip_file = zip_source_dir / zip_pattern
            else:
                zip_file = Path(zip_pattern)
            files = [str(zip_file)] if zip_file.exists() else []
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ–‡ä»¶ï¼Œå°è¯•å¸¸è§çš„æ¨¡å¼
        if not files and "*" not in zip_pattern:
            log(f"ğŸ’¡ æœªæ‰¾åˆ° {zip_pattern}ï¼Œå°è¯•å¸¸è§æ¨¡å¼...")
            common_patterns = [
                f"*{zip_pattern}*",
                f"{zip_pattern}_part*.zip",
                f"{Path(zip_pattern).stem}_part*.zip"
            ]
            
            for pattern in common_patterns:
                search_path = zip_source_dir / pattern
                files = glob(str(search_path))
                if files:
                    log(f"ğŸ’¡ è‡ªåŠ¨åŒ¹é…åˆ°æ¨¡å¼: {pattern}")
                    break
        
        return sorted(files)  # ç¡®ä¿é¡ºåºå¤„ç†
    
    log(f"ğŸ” åœ¨ç›®å½• {zip_source_dir} ä¸­æœç´¢: {zip_pattern}")
    zip_files = find_zip_files()
    
    if not zip_files:
        log(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„zipæ–‡ä»¶: {zip_pattern}")
        log(f"ğŸ’¡ è¯·æ£€æŸ¥:")
        log(f"  1. zipæ–‡ä»¶æ˜¯å¦åœ¨ {zip_source_dir} ç›®å½•ä¸­")
        log(f"  2. æ–‡ä»¶åæ¨¡å¼æ˜¯å¦æ­£ç¡®: {zip_pattern}")
        # åˆ—å‡ºç›®å½•ä¸­çš„zipæ–‡ä»¶
        existing_zips = list(zip_source_dir.glob("*.zip"))
        if existing_zips:
            log(f"  3. ç›®å½•ä¸­ç°æœ‰çš„zipæ–‡ä»¶:")
            for zf in existing_zips:
                log(f"     - {zf.name}")
        return extract_dir
    
    log(f"ğŸ“¦ æ‰¾åˆ° {len(zip_files)} ä¸ªzipæ–‡ä»¶ï¼Œå°†è§£å‹åˆ°: {extract_dir}")
    for i, zf in enumerate(zip_files, 1):
        size_mb = Path(zf).stat().st_size / (1024 * 1024) if Path(zf).exists() else 0
        log(f"  {i}. {Path(zf).name} ({size_mb:.1f} MB)")
    
    # æ™ºèƒ½è°ƒæ•´è¿›ç¨‹æ•°ï¼šä¸è¶…è¿‡zipæ–‡ä»¶æ•°é‡
    actual_workers = min(num_workers, len(zip_files))
    if actual_workers != num_workers:
        log(f"ğŸ’¡ è°ƒæ•´è¿›ç¨‹æ•°ä» {num_workers} åˆ° {actual_workers} (åŒ¹é…zipæ–‡ä»¶æ•°é‡)")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å·æ–‡ä»¶ï¼ˆé€šè¿‡æ–‡ä»¶åæ¨¡å¼åˆ¤æ–­ï¼‰
    is_volume_set = any("part" in Path(zf).name.lower() for zf in zip_files)
    if is_volume_set and len(zip_files) > 1:
        log(f"ğŸ” æ£€æµ‹åˆ°åˆ†å·æ–‡ä»¶ï¼Œä½¿ç”¨ä¼˜åŒ–çš„è§£å‹ç­–ç•¥")
    
    # å‡†å¤‡å¤šè¿›ç¨‹å‚æ•° - è§£å‹åˆ°extract_dir
    extract_tasks = [(zip_file, extract_dir) for zip_file in zip_files]
    
    # ä½¿ç”¨post_allocated_multiprocessè¿›è¡Œå¤šè¿›ç¨‹è§£å‹
    log(f"ğŸš€ å¼€å§‹å¤šè¿›ç¨‹è§£å‹ (ä½¿ç”¨ {actual_workers} ä¸ªè¿›ç¨‹)...")
    start_time = time.time()
    
    # ä½¿ç”¨post_allocated_multiprocessæ‰¹é‡å¤„ç†
    results = extract_zip_parallel(extract_tasks, num_workers=actual_workers)
    
    # ç»Ÿè®¡ç»“æœ
    total_files = 0
    total_size_mb = 0
    successful_extracts = 0
    
    for result in results:
        if result["status"] == "success":
            file_name = Path(result["file"]).name
            duration = result["duration"]
            size_mb = result["size_mb"]
            file_count = result["file_count"]
            process_id = result.get("process_id", "unknown")
            
            log(f"âœ… è¿›ç¨‹{process_id} - {file_name}: {file_count} ä¸ªæ–‡ä»¶, {size_mb:.1f}MB, {duration:.1f}ç§’")
            
            total_files += file_count
            total_size_mb += size_mb
            successful_extracts += 1
        else:
            process_id = result.get("process_id", "unknown")
            log(f"âŒ è¿›ç¨‹{process_id} - {Path(result['file']).name}: {result['message']}")
    
    total_duration = time.time() - start_time
    
    log(f"ğŸ‰ è§£å‹å®Œæˆ!")
    log(f"  âœ… æˆåŠŸ: {successful_extracts}/{len(zip_files)} ä¸ªæ–‡ä»¶")
    log(f"  ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
    log(f"  ğŸ’¾ æ€»å¤§å°: {total_size_mb:.1f} MB")
    log(f"  â±ï¸  æ€»è€—æ—¶: {total_duration:.1f} ç§’")
    log(f"  ğŸš€ å¹³å‡é€Ÿåº¦: {total_size_mb/total_duration:.1f} MB/s")
    log(f"  ğŸ“‚ ä»: {zip_source_dir}")
    log(f"  ğŸ“‚ åˆ°: {extract_dir}")
    
    # æ·»åŠ è§£å‹å®Œæ•´æ€§éªŒè¯
    if is_volume_set and successful_extracts == len(zip_files):
        log(f"ğŸ” éªŒè¯åˆ†å·è§£å‹å®Œæ•´æ€§...")
        verification_result = verify_extraction_completeness(zip_files, extract_dir)
        if verification_result["success"]:
            log(f"âœ… è§£å‹å®Œæ•´æ€§éªŒè¯é€šè¿‡: {verification_result['total_files']} ä¸ªæ–‡ä»¶")
        else:
            log(f"âš ï¸ è§£å‹å®Œæ•´æ€§éªŒè¯å¤±è´¥: {verification_result['message']}")
    
    return extract_dir


def verify_extraction_completeness(zip_files: List[str], extract_dir: Path) -> Dict[str, Any]:
    """éªŒè¯åˆ†å·è§£å‹çš„å®Œæ•´æ€§"""
    try:
        # ç»Ÿè®¡zipæ–‡ä»¶ä¸­çš„æ€»æ–‡ä»¶æ•°
        expected_files = set()
        total_expected = 0
        
        for zip_file in zip_files:
            with zipfile.ZipFile(zip_file, 'r') as zipf:
                for name in zipf.namelist():
                    if not name.endswith('/'):  # æ’é™¤ç›®å½•
                        expected_files.add(name)
                        total_expected += 1
        
        # ç»Ÿè®¡å®é™…è§£å‹çš„æ–‡ä»¶æ•°
        actual_files = set()
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                rel_path = os.path.relpath(os.path.join(root, file), extract_dir)
                actual_files.add(rel_path)
        
        missing_files = expected_files - actual_files
        extra_files = actual_files - expected_files
        
        if missing_files:
            return {
                "success": False,
                "message": f"ç¼ºå°‘ {len(missing_files)} ä¸ªæ–‡ä»¶",
                "missing_files": list(missing_files)[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
                "total_files": len(actual_files)
            }
        
        return {
            "success": True,
            "total_files": len(actual_files),
            "expected_files": len(expected_files)
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}",
            "total_files": 0
        }


@post_allocated_multiprocess
def process_jsonl_file(file_info, **kwargs):
    """å¹¶å‘å¤„ç†å•ä¸ªjsonlæ–‡ä»¶ä¸­çš„è·¯å¾„æ›´æ–°"""
    jsonl_file, dataset_dir, new_base_path = file_info
    
    try:
        updated_items = []
        
        with open(jsonl_file, "r") as f:
            lines = f.readlines()
        
        for line in lines:
            if not line.strip():
                continue
            
            try:
                item = json.loads(line)
                
                # æ›´æ–°åª’ä½“æ–‡ä»¶è·¯å¾„
                for media_type in ["images", "videos", "audios"]:
                    if media_type in item and item[media_type]:
                        updated_paths = []
                        for rel_path in item[media_type]:
                            # ä»ç›¸å¯¹è·¯å¾„æ„å»ºç»å¯¹è·¯å¾„
                            abs_path = new_base_path / dataset_dir.name / rel_path
                            updated_paths.append(str(abs_path))
                        item[media_type] = updated_paths
                
                updated_items.append(item)
                
            except json.JSONDecodeError as e:
                log(f"JSON decode error in {jsonl_file}: {e}")
                continue
        
        # å†™å›æ–‡ä»¶
        with open(jsonl_file, "w") as f:
            for item in updated_items:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        return {
            "file": str(jsonl_file),
            "items_count": len(updated_items),
            "status": "success"
        }
        
    except Exception as e:
        return {
            "file": str(jsonl_file),
            "status": "error", 
            "message": str(e)
        }


def update_yaml_config(yaml_file, new_base_path):
    """æ›´æ–°yamlé…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„"""
    with open(yaml_file, "r") as f:
        config = yaml.safe_load(f)
    
    # æ›´æ–° DataDir
    config["DataDir"] = str(new_base_path)
    
    # æ›´æ–°æ¯ä¸ªæ•°æ®é›†çš„MetaFilesè·¯å¾„
    updated_datasets = {}
    for dataset_name, dataset_config in config["Datasets"].items():
        metafiles_rel_path = dataset_config["MetaFiles"]
        metafiles_abs_path = new_base_path / metafiles_rel_path
        
        updated_datasets[dataset_name] = {
            "MetaFiles": str(metafiles_abs_path),
            "sample_nums": dataset_config["sample_nums"]
        }
    
    config["Datasets"] = updated_datasets
    
    # å†™å›æ–‡ä»¶
    with open(yaml_file, "w") as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    log(f"âœ… æ›´æ–°é…ç½®æ–‡ä»¶: {yaml_file}")
    return config


def unzip_update_dataset_paths(zip_pattern, zip_source_dir, new_base_path, num_workers=8):
    """è§£å‹zipæ–‡ä»¶å¹¶æ›´æ–°æ‰€æœ‰è·¯å¾„ä¸ºç»å¯¹è·¯å¾„"""
    zip_source_dir = Path(zip_source_dir)
    new_base_path = Path(new_base_path)
    
    log(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
    log(f"  ZIPæ¨¡å¼: {zip_pattern}")
    log(f"  ZIPæºç›®å½•: {zip_source_dir}")
    log(f"  è§£å‹åˆ°ç›®å½•: {new_base_path}")
    log(f"  è¿›ç¨‹æ•°é‡: {num_workers}")
    
    # å¤šè¿›ç¨‹è§£å‹zipæ–‡ä»¶ - ä»zip_source_dirè§£å‹åˆ°new_base_path
    dataset_dir = extract_zip_files(zip_pattern, zip_source_dir, new_base_path, num_workers)
    
    # æŸ¥æ‰¾yamlé…ç½®æ–‡ä»¶ - åœ¨è§£å‹åçš„ç›®å½•ä¸­æŸ¥æ‰¾
    yaml_files = list(dataset_dir.glob("*.yaml")) + list(dataset_dir.glob("*.yml"))
    if not yaml_files:
        log("âŒ æœªæ‰¾åˆ°yamlé…ç½®æ–‡ä»¶")
        return
    
    yaml_file = yaml_files[0]
    log(f"ğŸ“‹ å¤„ç†é…ç½®æ–‡ä»¶: {yaml_file}")
    
    # æ›´æ–°yamlé…ç½® - è·¯å¾„æŒ‡å‘new_base_path
    config = update_yaml_config(yaml_file, new_base_path)
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„jsonlæ–‡ä»¶
    jsonl_tasks = []
    for dataset_name in config["Datasets"].keys():
        dataset_subdir = dataset_dir / dataset_name
        if not dataset_subdir.exists():
            log(f"âš ï¸ æ•°æ®é›†å­ç›®å½•ä¸å­˜åœ¨: {dataset_subdir}")
            continue
        
        # æŸ¥æ‰¾MetaFilesç›®å½•ä¸­çš„jsonlæ–‡ä»¶
        metafiles_dir = dataset_subdir / "MetaFiles"
        if not metafiles_dir.exists():
            log(f"âš ï¸ MetaFilesç›®å½•ä¸å­˜åœ¨: {metafiles_dir}")
            continue
        
        jsonl_files = list(metafiles_dir.glob("*.jsonl"))
        for jsonl_file in jsonl_files:
            jsonl_tasks.append((jsonl_file, dataset_subdir, new_base_path))
    
    if not jsonl_tasks:
        log("âš ï¸ æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„jsonlæ–‡ä»¶")
        return
    
    log(f"ğŸ“ æ‰¾åˆ° {len(jsonl_tasks)} ä¸ªjsonlæ–‡ä»¶éœ€è¦æ›´æ–°è·¯å¾„")
    
    # å¤šè¿›ç¨‹æ›´æ–°jsonlæ–‡ä»¶è·¯å¾„
    log(f"ğŸš€ å¼€å§‹å¤šè¿›ç¨‹æ›´æ–°è·¯å¾„ (ä½¿ç”¨ {min(num_workers, len(jsonl_tasks))} ä¸ªè¿›ç¨‹)...")
    
    results = process_jsonl_file(
        jsonl_tasks, 
        num_workers=min(num_workers, len(jsonl_tasks))
    )
    
    # ç»Ÿè®¡ç»“æœ
    total_items = 0
    success_count = 0
    
    for result in results:
        if result["status"] == "success":
            items_count = result["items_count"]
            total_items += items_count
            success_count += 1
            log(f"  âœ… {Path(result['file']).name}: {items_count} ä¸ªæ•°æ®é¡¹")
        else:
            log(f"  âŒ {Path(result['file']).name}: {result['message']}")
    
    log(f"ğŸ‰ å®Œæˆ!")
    log(f"  ğŸ“¦ è§£å‹ä½ç½®: {new_base_path}")
    log(f"  âœ… æˆåŠŸæ–‡ä»¶: {success_count}/{len(jsonl_tasks)}")
    log(f"  ğŸ“Š æ€»æ•°æ®é¡¹: {total_items}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="è§£å‹æ•°æ®é›†zipæ–‡ä»¶å¹¶æ›´æ–°è·¯å¾„ä¸ºç»å¯¹è·¯å¾„ (æ”¯æŒå¤šè¿›ç¨‹)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è§£å‹ç›®å½•ä¸­çš„æ‰€æœ‰zipæ–‡ä»¶ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
  python scripts/unzip_repath_dataset.py \\
      --zip_source_dir "/home/data/zips" \\
      --target_dir "/data/extracted" \\
      --workers 8

  # è§£å‹ç‰¹å®šåˆ†å·zipæ–‡ä»¶
  python scripts/unzip_repath_dataset.py \\
      --zip_pattern "dataset_part*.zip" \\
      --zip_source_dir "/home/data/zips" \\
      --target_dir "/data/extracted" \\
      --workers 12
        """
    )
    
    parser.add_argument("--zip_pattern", default="*.zip",
                       help="zipæ–‡ä»¶åæ¨¡å¼ (é»˜è®¤: *.zipï¼Œæ”¯æŒé€šé…ç¬¦)")
    parser.add_argument("--zip_source_dir", required=True, help="zipæ–‡ä»¶æ‰€åœ¨ç›®å½•")
    parser.add_argument("--target_dir", required=True, help="è§£å‹ç›®æ ‡ç›®å½• (æ•°æ®æœ€ç»ˆå­˜æ”¾ä½ç½®)")
    parser.add_argument("--workers", type=int, default=8, help="å¹¶å‘è¿›ç¨‹æ•° (é»˜è®¤: 8ï¼Œä¼šè‡ªåŠ¨è°ƒæ•´ä¸ºzipæ–‡ä»¶æ•°é‡)")
    
    args = parser.parse_args()
    
    unzip_update_dataset_paths(args.zip_pattern, args.zip_source_dir, args.target_dir, args.workers)
