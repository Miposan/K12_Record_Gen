import os
import re
import sys
import yaml
import pickle as pkl
import random

from glob import glob
from copy import deepcopy
from omegaconf import OmegaConf
from typing import List, Literal, Tuple, Union
import base64
from PIL import Image
from io import BytesIO
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from tqdm import tqdm
import time

from datatool.logger import log
from datatool.utils.parallel import post_allocated_multiprocess, post_allocated_multithread, dynamic_task_pool_multiprocess
from datatool.utils.file_io import load_jsonlines, save_jsonlines_mpi, save_jsonlines, \
    load_video_bytes_base64, load_audio_bytes_base64, load_image_base64_with_type


def _build_tasks_for_one_file(args):
    """çº¯å‡½æ•°ï¼šç»™å®šä¸€ä¸ª metafileï¼Œè¿”å›è¯¥æ–‡ä»¶çš„æ‰€æœ‰ task åˆ—è¡¨"""
    metafile, target_metafile, dataset_name, skip_processed_items = args
    tasks = []
    
    # 1. å·²å¤„ç† id é›†åˆ
    processed_items = set()
    if os.path.exists(target_metafile):
        if skip_processed_items:
            try:
                processed_items = {item["id"] for item in load_jsonlines(target_metafile)}
            except Exception as e:
                log(f"Warn loading processed items from {target_metafile}: {e}")
        else:
            processed_items = set()

    
    # 2. è¯»è¯¥æ–‡ä»¶å…¨éƒ¨ item
    try:
        all_items = load_jsonlines(metafile)
    except Exception as e:
        log(f"Error loading {metafile}: {e}")
        return tasks
    
    # 3. ç”Ÿæˆ task
    for item in all_items:
        if item["id"] not in processed_items:
            tasks.append([
                item,
                metafile,          # src_path
                target_metafile,   # save_path
                dataset_name
            ])
    return tasks

def remove_file(path):
    try:
        if os.path.exists(path):
            os.remove(path)
            return True
    except Exception as e:
        log(f"åˆ é™¤æ–‡ä»¶ {path} å¤±è´¥: {e}", level=log.WARNING)
    return False


def process_metafiles_hook(dataset_yaml_path,
                           hook_func,
                           dst_metafiles_name=None,
                           skip_processed_items=True,
                           write_mode: Literal['item', 'file'] = 'item',
                           num_workers=1,
                           **hook_kwargs):
    """
    æ ¹æ®è¾“å…¥çš„é’©å­å‡½æ•°ï¼ˆhook_funcï¼‰ï¼Œä» `dataset_yaml_path` ä¸­çš„ MetaFiles æ–‡ä»¶ä¸­æå–ç›®æ ‡æ•°æ®åˆ° `dst_metafiles_name` æ–‡ä»¶å¤¹ä¸‹ã€‚

    # é’©å­å‡½æ•°ç¤ºä¾‹ -- æå–å‡ºå¤šè½®å¯¹è¯æ•°æ®
    def extract_multivqa(item):
        '''
        Args:
            item (dict): å•ä¸ª unistore æ ¼å¼çš„æ ‡å‡† json æ ¼å¼æ•°æ®
        
        Returns:
            bool: True è¡¨ç¤ºä¿ç•™æ•°æ®ï¼ŒFalse è¡¨ç¤ºä¸¢å¼ƒæ•°æ®
            item: ä¿ç•™çš„æ•°æ®ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªè¿”å›å€¼ä¸º True æ—¶ç”Ÿæ•ˆï¼‰
        '''
        if len(item["conversations"]) > 2:
            yield item

    Args:
        dataset_yaml_path (str): æºæ•°æ®æ¡¶çš„ yaml æ–‡ä»¶è·¯å¾„ï¼Œæ³¨æ„æ–‡ä»¶çº§åˆ«çš„å¹¶è¡Œåªæ”¯æŒä¼ è¿›æ¥ä¸ºyamlçš„å½¢å¼
        hook_func (function): é’©å­å‡½æ•°è¿­ä»£å™¨ï¼Œè¯¥å‡½æ•°æ¥æ”¶ä¸€ä¸ªå­—å…¸ç±»å‹çš„å‚æ•°ï¼Œå°†éœ€è¦ä¿å­˜çš„æ•°æ®ä»¥è¿­ä»£å™¨å½¢å¼è¿”å›
        dst_metafiles_name (str): æ–°æ•°æ®å­˜å‚¨ç›®å½•çš„åç§°ï¼Œè‹¥ä¸º Noneï¼Œåˆ™åŸåœ°ä¿®æ”¹åŸå§‹æ–‡ä»¶
        skip_processed_items (bool, optional): æ˜¯å¦è·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ•°æ®ã€‚è‹¥ä¸º Falseï¼Œå°†ä¼šåˆ é™¤å·²å¤„ç†è¿‡çš„æ•°æ®ã€‚Defaults to True
        write_mode (Literal['item', 'file'], optional): å†™å…¥æ¨¡å¼ï¼Œ'item'è¡¨ç¤ºé€è¡Œå¤„ç†å¹¶å†™å…¥ï¼Œ'file'è¡¨ç¤ºæ‰¹é‡å¤„ç†æ•´ä¸ªæ–‡ä»¶åå†™å…¥ã€‚Defaults to 'item'
        num_workers (int, optional): å¤šè¿›ç¨‹çš„ worker æ•°é‡ã€‚Defaults to 1
    """
    @post_allocated_multiprocess
    def _process(task, **kwargs):
        dataset_name, src_path, save_path = task
        # åŠ è½½å·²å¤„ç†è¿‡çš„æ•°æ®
        if os.path.exists(save_path) and skip_processed_items:
            if write_mode == "item":
                processed_items = set([item["id"] for item in load_jsonlines(save_path)])
                log(f"found {processed_items} items in caches.")
            else:
                # è·³è¿‡å·²å¤„ç†å®Œçš„æ–‡ä»¶
                log(f"skip {save_path} due to processed cache.")
                return None
        else:
            if os.path.exists(save_path):
                os.remove(save_path)
            processed_items = set()
        all_new_data = []
        log(f"Processing {src_path}...")
        for item in load_jsonlines(src_path):
            if item["id"] in processed_items:
                continue
            for new_data in hook_func(
                    deepcopy(item), 
                    dataset_name=dataset_name,
                    src_path=src_path,
                    **hook_kwargs
                ):
                if write_mode == "item":
                    save_jsonlines([new_data], save_path, mode="a")
                else:
                    all_new_data.append(new_data)
        if write_mode != "item" and len(all_new_data) > 0:
            save_jsonlines(all_new_data, save_path, mode="w")
        return None
        
    if not os.path.exists(dataset_yaml_path):
        raise FileNotFoundError(f"Dataset yaml file {dataset_yaml_path} not found.")

    assert write_mode in ['item', 'file'], f"write_mode should be in ['item', 'file'], but got {write_mode}"

    all_tasks = []
    dataset_config = OmegaConf.load(dataset_yaml_path)
    for dataset_name, config in dataset_config["Datasets"].items():
        metafile_dir = config["MetaFiles"]
        if not os.path.exists(metafile_dir):
            log(f"skip metafile_dir not exist: {metafile_dir}")
            continue
        all_metafiles = glob(
            os.path.join(metafile_dir, "**", "*.jsonl"), recursive=True)
        # å¢åŠ å½“å‰æ•°æ®é›†ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ä»»åŠ¡
        if dst_metafiles_name == os.path.basename(metafile_dir):
            log(f"{dst_metafiles_name} has existed, please use a new metafile dirname.", level=log.ERROR)
            return
        for metafile in all_metafiles:
            try:
                with open(metafile, 'r', encoding='utf-8') as f:
                    has_valid_line = False
                    for line in f:
                        line = line.strip()
                        if line:
                            has_valid_line = True
                            break
                if not has_valid_line:
                    log(f"skip empty metafile: {metafile}")
                    continue
            except Exception as e:
                log(f"skip invalid metafile: {metafile}, error: {e}")
                continue
            all_tasks.append([
                dataset_name,
                metafile, 
                os.path.join(os.path.dirname(metafile_dir),
                            dst_metafiles_name, os.path.relpath(metafile, metafile_dir))
            ])
    log(f"Found {len(all_tasks)} metafiles in all.")
    _process(all_tasks, num_workers=num_workers)
    log("Done.")



def _process_yaml_items(dataset_yaml_path, hook_func, dst_metafiles_name, skip_processed_items, num_workers, **hook_kwargs):
    """
    å¤„ç†yamlé…ç½®ï¼Œitemçº§å¹¶è¡Œ
    """
    @post_allocated_multiprocess
    def _process(task, **kwargs):
        item, src_path, save_path, dataset_name = task
        for new_data in hook_func(deepcopy(item), 
                                  dataset_name=dataset_name, 
                                  src_path=src_path,
                                  save_path=save_path,
                                  **hook_kwargs):
            save_jsonlines_mpi([new_data], save_path, mode="a")
        return None
    # æ—¶é—´ç‚¹ï¼šä»»åŠ¡æ”¶é›†é˜¶æ®µ
    task_collection_start = time.time()
    all_tasks = [] # æ¯ä¸ª item ä¸€ä¸ªä»»åŠ¡
    target_metafile_list = []
    dataset_config = OmegaConf.load(dataset_yaml_path)
# ğŸš€ ä¼˜åŒ–1: é¢„æ”¶é›†æ‰€æœ‰metafileä¿¡æ¯ï¼Œé¿å…é‡å¤è®¡ç®—
    all_metafiles_info = []
    for dataset_name, config in dataset_config["Datasets"].items():
        metafile_dir = config["MetaFiles"]
        all_metafiles = glob(
            os.path.join(metafile_dir, "**", "*.jsonl"), recursive=True)
        
        # æ£€æŸ¥ç›®æ ‡ç›®å½•
        if dst_metafiles_name == os.path.basename(metafile_dir):
            log(f"{dst_metafiles_name} has existed, please use a new metafile dirname.", level=log.ERROR)
            return
        
        # é¢„æ”¶é›†metafileä¿¡æ¯
        for metafile in all_metafiles:
            if os.path.exists(metafile):
                target_metafile = metafile.replace(
                    metafile_dir, 
                    os.path.join(os.path.dirname(metafile_dir), dst_metafiles_name)
                )
                all_metafiles_info.append((metafile, target_metafile, dataset_name))
    
    log(f"Found {len(all_metafiles_info)} metafiles to process")
    
    # å½“ä¸è·³è¿‡å·²å¤„ç†é¡¹æ—¶ï¼Œä¸»è¿›ç¨‹å…ˆè¡Œåˆ é™¤å·²å­˜åœ¨çš„ç›®æ ‡æ–‡ä»¶ï¼Œç»Ÿä¸€æ‰“æ—¥å¿—
    if not skip_processed_items:
        deleted_count = 0
        for _, target_metafile, _ in all_metafiles_info:
            target_dir = os.path.dirname(target_metafile)
            if not os.path.exists(target_dir):
                continue
            if os.path.exists(target_metafile):
                log(f"delete {target_metafile}")
                try:
                    os.remove(target_metafile)
                    deleted_count += 1
                except Exception as e:
                    log(f"failed to delete {target_metafile}: {e}", level=log.WARNING)
        if deleted_count:
            log(f"deleted {deleted_count} existing target metafiles before collection")
    
    # ğŸš€ ä¼˜åŒ–2: å¹¶è¡Œæ”¶é›†ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
    
    # å¹¶è¡Œæ”¶é›†ä»»åŠ¡
    log("Collecting tasks in parallel ...")
    with ProcessPoolExecutor(max_workers=min(8, len(all_metafiles_info))) as ex:
        # å…ˆæŠŠå‚æ•°æ‹å¹³
        job_args = [
            (metafile, target_metafile, dataset_name, skip_processed_items)
            for metafile, target_metafile, dataset_name in all_metafiles_info
        ]
        # æäº¤
        future_to_file = {ex.submit(_build_tasks_for_one_file, arg): arg[0] for arg in job_args}
        
        # æ·»åŠ è¿›åº¦æ¡æ˜¾ç¤ºå¹¶è¡Œæ”¶é›†è¿›åº¦
        with tqdm(total=len(job_args), desc="Collecting tasks") as pbar:
            for fut in as_completed(future_to_file):
                file_tasks = fut.result()
                all_tasks.extend(file_tasks)
                
                # åŒæ—¶æ”¶é›†target_metafile_list
                if file_tasks:
                    # ä»ç¬¬ä¸€ä¸ªtaskä¸­è·å–target_metafile
                    target_metafile = file_tasks[0][2]  # save_path
                    target_metafile_list.append(target_metafile)
                
                pbar.update(1)
                pbar.set_postfix({"total_tasks": len(all_tasks)})
        
        log(f"Collected {len(all_tasks)} tasks from {len(all_metafiles_info)} files")
    
    task_collection_time = time.time() - task_collection_start
    log(f"[PERF-MAIN] Task collection took: {task_collection_time:.3f}s")
    log(f"Found {len(all_tasks)} items in all metafiles.")

    # å¤šè¿›ç¨‹æ–¹å¼è·å–æ¯æ¡æ•°æ®çš„ predicts
    execution_start = time.time()
    log(f"[PERF-MAIN] Starting ProcessPoolExecutor with {num_workers} workers...")
    _process(all_tasks, num_workers=min(num_workers, len(all_tasks)))
    execution_time = time.time() - execution_start
    log(f"[PERF-MAIN] ProcessPool execution took: {execution_time:.3f}s")
    log(f"[PERF-MAIN] Average time per task: {execution_time/len(all_tasks):.3f}s")
    log("Done.")

    # ç»Ÿè®¡æ‰€æœ‰ç”Ÿæˆçš„æ•°æ®æ¡æ•°
    total_count = 0
    folder_counts = {}
    for dataset_name, config in dataset_config["Datasets"].items():
        metafile_dir = config["MetaFiles"]
        if dst_metafiles_name is not None:
            save_root = os.path.join(os.path.dirname(metafile_dir), dst_metafiles_name)
            folder_count = 0
            for root, dirs, files in os.walk(save_root):
                for file in files:
                    if file.endswith('.jsonl'):
                        file_path = os.path.join(root, file)
                        try:
                            count = sum(1 for _ in open(file_path, 'r', encoding='utf-8'))
                            folder_count += count
                        except Exception as e:
                            log(f"[ç»Ÿè®¡] ç»Ÿè®¡ {file_path} å¤±è´¥: {e}")
            folder_counts[save_root] = folder_count
            total_count += folder_count
    for folder, count in folder_counts.items():
        log(f"[ç»Ÿè®¡] è½¬æ¢åæ•°æ®æ€»æ•°: {count} in {folder}")
    log(f"[ç»Ÿè®¡] æ‰€æœ‰æ–‡ä»¶å¤¹æ€»æ•°: {total_count}")

    # åˆ é™¤ä¸´æ—¶çš„é”æ–‡ä»¶
    log("åˆ é™¤ä¸´æ—¶çš„æ–‡ä»¶é”...")
    # æ”¶é›†æ‰€æœ‰éœ€è¦æ¸…ç†çš„ç›®å½•ï¼Œé¿å…é‡å¤å¤„ç†
    unique_dirs = set()
    for target_metafile in target_metafile_list:
        target_metafile_dir = os.path.dirname(target_metafile)
        unique_dirs.add(target_metafile_dir)
    
    log(f"éœ€è¦æ¸…ç†é”æ–‡ä»¶çš„ç›®å½•æ•°é‡: {len(unique_dirs)}")
    
    # æ”¶é›†æ‰€æœ‰é”æ–‡ä»¶
    all_lock_files = []
    for target_metafile_dir in unique_dirs:
        if not os.path.exists(target_metafile_dir):
            log(f"ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {target_metafile_dir}")
            continue
        try:
            locks = glob(
                os.path.join(target_metafile_dir, "**", "*.jsonl.lock"), recursive=True
            )
            if locks:
                all_lock_files.extend(locks)
                log(f"æ‰¾åˆ° {len(locks)} ä¸ªé”æ–‡ä»¶ in {target_metafile_dir}")
            else:
                log(f"ä¸å­˜åœ¨é”æ–‡ä»¶ in {target_metafile_dir}")
                return
        except Exception as e:
            log(f"æœç´¢ç›®å½• {target_metafile_dir} é”æ–‡ä»¶æŠ¥é”™: {e}", level=log.WARNING)

    log(f"å¾…åˆ é™¤é”æ–‡ä»¶æ€»æ•°: {len(all_lock_files)}")
    
    if len(all_lock_files) == 0:
        return

    # å¤šçº¿ç¨‹å¹¶å‘åˆ é™¤æ–‡ä»¶
    total_removed = 0
    with ThreadPoolExecutor(max_workers=min(4, len(all_lock_files))) as executor:
        futures = {executor.submit(remove_file, f): f for f in all_lock_files}
        for i, future in enumerate(as_completed(futures), 1):
            if future.result():
                total_removed += 1
            if i % 1000 == 0:
                log(f"å·²åˆ é™¤ {i} ä¸ªé”æ–‡ä»¶...")

    log(f"é”æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {total_removed} ä¸ªæ–‡ä»¶")
    log("Done.")


def process_items_hook_multithreads(dataset_yaml_path,
                                    hook_func,
                                    dst_metafiles_name=None,
                                    skip_processed_items=True,
                                    num_workers=1,
                                    **hook_kwargs):
    """
    æ ¹æ®è¾“å…¥çš„é’©å­å‡½æ•°ï¼ˆhook_funcï¼‰ï¼Œä» `dataset_yaml_path` ä¸­çš„ MetaFiles æ–‡ä»¶ä¸­æå–ç›®æ ‡æ•°æ®åˆ° `dst_metafiles_name` æ–‡ä»¶å¤¹ä¸‹ã€‚
    ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†ï¼Œé€‚ç”¨äºI/Oå¯†é›†å‹ä»»åŠ¡ã€‚

    # é’©å­å‡½æ•°ç¤ºä¾‹ -- æå–å‡ºå¤šè½®å¯¹è¯æ•°æ®
    def extract_multivqa(item):
        '''
        Args:
            item (dict): å•ä¸ª unistore æ ¼å¼çš„æ ‡å‡† json æ ¼å¼æ•°æ®
        
        Returns:
            bool: True è¡¨ç¤ºä¿ç•™æ•°æ®ï¼ŒFalse è¡¨ç¤ºä¸¢å¼ƒæ•°æ®
            item: ä¿ç•™çš„æ•°æ®ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªè¿”å›å€¼ä¸º True æ—¶ç”Ÿæ•ˆï¼‰
        '''
        if len(item["conversations"]) > 2:
            yield item

    Args:
        dataset_yaml_path (str): æºæ•°æ®æ¡¶çš„ yaml æ–‡ä»¶è·¯å¾„
        hook_func (function): é’©å­å‡½æ•°è¿­ä»£å™¨ï¼Œè¯¥å‡½æ•°æ¥æ”¶ä¸€ä¸ªå­—å…¸ç±»å‹çš„å‚æ•°ï¼Œå°†éœ€è¦ä¿å­˜çš„æ•°æ®ä»¥è¿­ä»£å™¨å½¢å¼è¿”å›
        dst_metafiles_name (str): æ–°æ•°æ®å­˜å‚¨ç›®å½•çš„åç§°ï¼Œè‹¥ä¸º Noneï¼Œåˆ™åŸåœ°ä¿®æ”¹åŸå§‹æ–‡ä»¶
        skip_processed_items (bool, optional): æ˜¯å¦è·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ•°æ®ã€‚è‹¥ä¸º Falseï¼Œå°†ä¼šåˆ é™¤å·²å¤„ç†è¿‡çš„æ•°æ®ã€‚Defaults to True
        num_workers (int, optional): å¤šçº¿ç¨‹çš„ worker æ•°é‡ã€‚Defaults to 1
    """
    def single_process(task):
        item, src_path, save_path, dataset_name = task
        for new_data in hook_func(
                deepcopy(item), 
                src_path=src_path,
                save_path=save_path,
                dataset_name=dataset_name,
                **hook_kwargs
            ):
            if new_data:
                save_jsonlines_mpi([new_data], save_path, mode="a")
        return None
    @post_allocated_multithread
    def _process(task, **kwargs):
        item, src_path, save_path, dataset_name = task
        for new_data in hook_func(
                deepcopy(item), 
                src_path=src_path,
                save_path=save_path,
                dataset_name=dataset_name,
                **hook_kwargs
            ):
            if new_data:
                save_jsonlines_mpi([new_data], save_path, mode="a")
        return None
        
    if not os.path.exists(dataset_yaml_path):
        raise FileNotFoundError(f"Dataset yaml file {dataset_yaml_path} not found.")

    # æ—¶é—´ç‚¹ï¼šä»»åŠ¡æ”¶é›†é˜¶æ®µ
    task_collection_start = time.time()
    all_tasks = [] # æ¯ä¸ª item ä¸€ä¸ªä»»åŠ¡
    target_metafile_list = []
    dataset_config = OmegaConf.load(dataset_yaml_path)
    
    # ğŸš€ ä¼˜åŒ–1: é¢„æ”¶é›†æ‰€æœ‰metafileä¿¡æ¯ï¼Œé¿å…é‡å¤è®¡ç®—
    all_metafiles_info = []
    for dataset_name, config in dataset_config["Datasets"].items():
        metafile_dir = config["MetaFiles"]
        all_metafiles = glob(
            os.path.join(metafile_dir, "**", "*.jsonl"), recursive=True)
        
        # æ£€æŸ¥ç›®æ ‡ç›®å½•
        if dst_metafiles_name == os.path.basename(metafile_dir):
            log(f"{dst_metafiles_name} has existed, please use a new metafile dirname.", level=log.ERROR)
            return
        
        # é¢„æ”¶é›†metafileä¿¡æ¯
        for metafile in all_metafiles:
            if os.path.exists(metafile):
                target_metafile = metafile.replace(
                    metafile_dir, 
                    os.path.join(os.path.dirname(metafile_dir), dst_metafiles_name)
                )
                all_metafiles_info.append((metafile, target_metafile, dataset_name))
    
    log(f"Found {len(all_metafiles_info)} metafiles to process")
    
    # å½“ä¸è·³è¿‡å·²å¤„ç†é¡¹æ—¶ï¼Œä¸»è¿›ç¨‹å…ˆè¡Œåˆ é™¤å·²å­˜åœ¨çš„ç›®æ ‡æ–‡ä»¶ï¼Œç»Ÿä¸€æ‰“æ—¥å¿—
    if not skip_processed_items:
        deleted_count = 0
        for _, target_metafile, _, _ in all_metafiles_info:
            target_dir = os.path.dirname(target_metafile)
            if not os.path.exists(target_dir):
                continue
            if os.path.exists(target_metafile):
                log(f"delete {target_metafile}")
                try:
                    os.remove(target_metafile)
                    deleted_count += 1
                except Exception as e:
                    log(f"failed to delete {target_metafile}: {e}", level=log.WARNING)
        if deleted_count:
            log(f"deleted {deleted_count} existing target metafiles before collection")
    
    # ğŸš€ ä¼˜åŒ–2: å¹¶è¡Œæ”¶é›†ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
    
    # å¹¶è¡Œæ”¶é›†ä»»åŠ¡
    log("Collecting tasks in parallel ...")
    with ProcessPoolExecutor(max_workers=min(8, len(all_metafiles_info))) as ex:
        # å…ˆæŠŠå‚æ•°æ‹å¹³
        job_args = [
            (metafile, target_metafile, dataset_name, skip_processed_items)
            for metafile, target_metafile, dataset_name in all_metafiles_info
        ]
        # æäº¤
        future_to_file = {ex.submit(_build_tasks_for_one_file, arg): arg[0] for arg in job_args}
        # æ±‡æ€»
        # all_tasks = []
        target_metafile_list = []
        
        # æ·»åŠ è¿›åº¦æ¡æ˜¾ç¤ºå¹¶è¡Œæ”¶é›†è¿›åº¦
        with tqdm(total=len(job_args), desc="Collecting tasks") as pbar:
            for fut in as_completed(future_to_file):
                file_tasks = fut.result()
                all_tasks.extend(file_tasks)
                
                # åŒæ—¶æ”¶é›†target_metafile_list
                if file_tasks:
                    # ä»ç¬¬ä¸€ä¸ªtaskä¸­è·å–target_metafile
                    target_metafile = file_tasks[0][2]  # save_path
                    target_metafile_list.append(target_metafile)
                
                pbar.update(1)
                pbar.set_postfix({"total_tasks": len(all_tasks)})
        
        log(f"Collected {len(all_tasks)} tasks from {len(all_metafiles_info)} files")
    
    task_collection_time = time.time() - task_collection_start
    log(f"[PERF-MAIN] Task collection took: {task_collection_time:.3f}s")
    log(f"Found {len(all_tasks)} items in all metafiles.")

    
    # å¤šçº¿ç¨‹æ–¹å¼è·å–æ¯æ¡æ•°æ®çš„ predicts
    log(f"[PERF-MAIN] Starting ThreadPoolExecutor with {num_workers} workers...")
    execution_start = time.time()
    _process(all_tasks, num_workers=min(num_workers, len(all_tasks)))

    # with ThreadPoolExecutor(max_workers=num_workers) as pool:
    #     futures = {pool.submit(single_process, sample): sample for sample in all_tasks}
    #     # æ·»åŠ è¿›åº¦æ¡
    #     with tqdm(total=len(all_tasks), desc="Processing") as pbar:
    #         for _ in as_completed(futures):
    #             pbar.update(1)
    
    execution_time = time.time() - execution_start
    log(f"[PERF-MAIN] ProcessPool execution took: {execution_time:.3f}s")
    log(f"[PERF-MAIN] Average time per task: {execution_time/len(all_tasks):.3f}s")
    
    # åˆ é™¤ä¸´æ—¶çš„é”æ–‡ä»¶
    log("åˆ é™¤ä¸´æ—¶çš„æ–‡ä»¶é”...")
    # æ”¶é›†æ‰€æœ‰éœ€è¦æ¸…ç†çš„ç›®å½•ï¼Œé¿å…é‡å¤å¤„ç†
    unique_dirs = set()
    for target_metafile in target_metafile_list:
        target_metafile_dir = os.path.dirname(target_metafile)
        unique_dirs.add(target_metafile_dir)
    
    log(f"éœ€è¦æ¸…ç†é”æ–‡ä»¶çš„ç›®å½•æ•°é‡: {len(unique_dirs)}")
    
    # æ”¶é›†æ‰€æœ‰é”æ–‡ä»¶
    all_lock_files = []
    for target_metafile_dir in unique_dirs:
        if not os.path.exists(target_metafile_dir):
            log(f"ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {target_metafile_dir}")
            continue
        try:
            locks = glob(
                os.path.join(target_metafile_dir, "**", "*.jsonl.lock"), recursive=True
            )
            if locks:
                all_lock_files.extend(locks)
                log(f"æ‰¾åˆ° {len(locks)} ä¸ªé”æ–‡ä»¶ in {target_metafile_dir}")
        except Exception as e:
            log(f"æœç´¢ç›®å½• {target_metafile_dir} é”æ–‡ä»¶æŠ¥é”™: {e}", level=log.WARNING)

    log(f"å¾…åˆ é™¤é”æ–‡ä»¶æ€»æ•°: {len(all_lock_files)}")

    # å¤šçº¿ç¨‹å¹¶å‘åˆ é™¤æ–‡ä»¶
    total_removed = 0
    if len(all_lock_files) == 0:
        return
    with ThreadPoolExecutor(max_workers=min(16, len(all_lock_files))) as executor:
        futures = {executor.submit(remove_file, f): f for f in all_lock_files}
        for i, future in enumerate(as_completed(futures), 1):
            if future.result():
                total_removed += 1
            if i % 1000 == 0:
                log(f"å·²åˆ é™¤ {i} ä¸ªé”æ–‡ä»¶...")

    log(f"é”æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {total_removed} ä¸ªæ–‡ä»¶")
    log("Done.")



def process_data_hook(dataset_path,
                       hook_func,
                       dst_metafiles_name=None,
                       skip_processed_items=True,
                       num_workers=1,
                       parallel_level="item",
                       parallel_type="process",
                       **hook_kwargs):
    """
    é€šç”¨å…¥å£ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼å’Œå¹¶è¡Œç²’åº¦ï¼Œæ”¯æŒå¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹ä¸¤ç§å¹¶è¡Œæ–¹å¼
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        hook_func: é’©å­å‡½æ•°
        dst_metafiles_name: ç›®æ ‡æ–‡ä»¶å¤¹åç§°
        skip_processed_items: æ˜¯å¦è·³è¿‡å·²å¤„ç†è¿‡çš„æ•°æ®ï¼Œé»˜è®¤True
        num_workers: å¹¶è¡Œè¿›ç¨‹/çº¿ç¨‹æ•°
        parallel_level: "item"ï¼ˆé»˜è®¤ï¼Œå•æ¡æ•°æ®å¹¶è¡Œï¼‰æˆ–"file"ï¼ˆæ–‡ä»¶çº§å¹¶è¡Œï¼‰
        parallel_type: "process"ï¼ˆå¤šè¿›ç¨‹ï¼Œé»˜è®¤ï¼‰æˆ–"thread"ï¼ˆå¤šçº¿ç¨‹ï¼‰
        hook_kwargs: é’©å­å‡½æ•°å‚æ•°
    
    ä½¿ç”¨åœºæ™¯è¯´æ˜:
    - å¤šè¿›ç¨‹(process): é€‚ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡ï¼Œå¦‚å¤æ‚çš„æ•°æ®å˜æ¢ã€æ¨¡å‹æ¨ç†ç­‰
    - å¤šçº¿ç¨‹(thread): é€‚ç”¨äºI/Oå¯†é›†å‹ä»»åŠ¡ï¼Œå¦‚æ–‡ä»¶è¯»å†™ã€ç½‘ç»œè¯·æ±‚ã€ç®€å•çš„æ•°æ®å¤„ç†ç­‰
    
    å¹¶è¡Œçº§åˆ«è¯´æ˜:
    - itemçº§å¹¶è¡Œ: å¯¹æ¯ä¸ªæ•°æ®é¡¹è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œé€‚åˆæ•°æ®é‡å¤§çš„åœºæ™¯
    - fileçº§å¹¶è¡Œ: å¯¹æ¯ä¸ªæ–‡ä»¶è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œé€‚åˆæ–‡ä»¶æ•°é‡å¤šä½†å•æ–‡ä»¶æ•°æ®é‡é€‚ä¸­çš„åœºæ™¯
    """
    SUPPORTED_FORMATS = {'.yaml': _process_yaml_items, '.yml': _process_yaml_items}
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"Dataset yaml/jsonl file {dataset_path} not found.")
    ext = os.path.splitext(dataset_path)[-1].lower()
    if ext not in SUPPORTED_FORMATS:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
    
    if parallel_level == "item":
        if parallel_type == "process":
            # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
            process_func = SUPPORTED_FORMATS[ext]
            process_func(dataset_path, hook_func, dst_metafiles_name, skip_processed_items, num_workers, **hook_kwargs)
        elif parallel_type == "thread":
            # ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç† - ä»…æ”¯æŒyamlæ ¼å¼
            #if not dataset_path.endswith(('.yaml', '.yml')):
            #    raise ValueError("å¤šçº¿ç¨‹æ¨¡å¼ç›®å‰ä»…æ”¯æŒyamlæ–‡ä»¶")
            process_items_hook_multithreads(dataset_path, hook_func, dst_metafiles_name, skip_processed_items, num_workers, **hook_kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹¶è¡Œç±»å‹: {parallel_type}ï¼Œæ”¯æŒçš„ç±»å‹: ['process', 'thread']")
    elif parallel_level == "file":
        # fileçº§å¹¶è¡Œå¿…é¡»ä¼ è¿›æ¥çš„æ˜¯yamlæ–‡ä»¶ï¼Œä¸”åªæ”¯æŒå¤šè¿›ç¨‹
        if not dataset_path.endswith(('.yaml', '.yml')):
            raise ValueError("æ–‡ä»¶çº§å¹¶è¡Œå¿…é¡»ä¼ è¿›æ¥çš„æ˜¯yamlæ–‡ä»¶")
        if parallel_type != "process":
            log(f"æ–‡ä»¶çº§å¹¶è¡Œå¼ºåˆ¶ä½¿ç”¨å¤šè¿›ç¨‹æ¨¡å¼ï¼Œå¿½ç•¥parallel_type={parallel_type}", level=log.WARNING)
        process_metafiles_hook(dataset_path, hook_func, dst_metafiles_name, skip_processed_items, 
                              write_mode='file', num_workers=num_workers, **hook_kwargs)
    else:
        raise NotImplementedError(f"ä¸æ”¯æŒ {parallel_level}çº§åˆ«å¹¶è¡Œ")


def get_image_base64(image_path, new_width=None, new_height=None):
  """
  function: æœ¬åœ°å›¾ç‰‡è·¯å¾„è½¬åŒ–æˆurl
  image_path: str å›¾ç‰‡è·¯å¾„
  """
  # è¯»å–å›¾ç‰‡
  with open(image_path, "rb") as img:
    # è°ƒæ•´å›¾ç‰‡å°ºå¯¸
    if new_width and new_height:
      with Image.open(image_path, "r") as img:
          img = img.resize((new_width, new_height))
          img = img.convert("RGB")
          # å°†å›¾ç‰‡ä¿å­˜åˆ°å­—èŠ‚æµä¸­ï¼Œè€Œä¸æ˜¯æ–‡ä»¶ä¸­
          buffered = BytesIO()
          img.save(buffered, format="JPEG")

      # è·å–Base64ç¼–ç 
      base64_encoded = base64.b64encode(buffered.getvalue()).decode('utf-8')
    else:
      base64_encoded = base64.b64encode(img.read()).decode("utf-8")

  # æ„å»ºdata URL
  base64_url = f"data:image/jpeg;base64,{base64_encoded}"
  return base64_url


# æå–è§†é¢‘
def get_video_base64(video_path):
    with open(video_path, 'rb') as video_file:
        video_base = base64.b64encode(video_file.read()).decode('utf-8')
    base64_url = f"data:image/jpeg;base64,{video_base}"
    return base64_url

def get_audio_base64(audio_path):
    # å…ˆç©ºå®ç°
    return None

def replace_media_path(path, source_dir, target_dir):
    if source_dir and target_dir and path.startswith(source_dir):
        return path.replace(source_dir, target_dir, 1)
    return path

def load_message_from_data(
        meta_item, 
        load_media: bool = True, 
        system_prompt: str = None,
        merge_answer: bool = False,
        source_dir: str = None,
        target_dir: str = None,
        media_to_base64 : bool = True
    ) -> Union[List, Tuple[List, str]]:
    """
    å¤„ç†å¤šæ¨¡æ€å¯¹è¯æ•°æ®ï¼Œæ”¯æŒ<image>ã€<video>ã€<audio>å ä½ç¬¦æ›¿æ¢ä¸ºbase64å†…å®¹
    æ”¯æŒsource_dir/target_dirè·¯å¾„æ›¿æ¢ï¼ˆè€ƒè™‘åˆ°mediaæ–‡ä»¶è€æ˜¯è¿ç§»ï¼Œæ‰€ä»¥éœ€è¦æ”¯æŒè·¯å¾„æ›¿æ¢ï¼‰
    """
    import uuid
    messages = []
    images = meta_item.get('images', [])
    videos = meta_item.get('videos', [])
    audios = meta_item.get('audios', [])

    image_idx, video_idx, audio_idx = 0, 0, 0

    conversations = meta_item['messages']
    if system_prompt is not None:
        messages.append({"role": "system", "content": [{"type": "text", "text": system_prompt}]})
    for idx, conv in enumerate(conversations):
        role = conv['role']
        content = []
        conv_text = conv['content']

        if role == 'user':
            last_idx = 0
            for match in re.finditer(r'<(image|video|audio)>', conv_text):
                media_type = match.group(1)
                start, end = match.span()
                if start > last_idx:
                    content.append({"type": "text", "text": conv_text[last_idx:start]})
                if media_type == "image" and image_idx < len(images):
                    img_path = replace_media_path(images[image_idx], source_dir, target_dir)
                    if load_media:
                        if media_to_base64:
                            img64, img_type = load_image_base64_with_type(img_path)
                            content.append({"type": "image_url", "image_url": {"url": f"data:{img_type};base64,{img64}"}})
                        else:
                            content.append({"type": "image_url", "image_url": {"url": img_path}})
                    image_idx += 1
                elif media_type == "video" and video_idx < len(videos):
                    vid_path = replace_media_path(videos[video_idx], source_dir, target_dir)
                    if load_media:
                        if media_to_base64:
                            vid64 = load_video_bytes_base64(vid_path)
                            content.append({"type": "video_url", "video_url": {"url": f"data:video/mp4;base64,{vid64}"}})
                        else:
                            content.append({"type": "video_url", "video_url": {"url": vid_path}})
                    video_idx += 1
                elif media_type == "audio" and audio_idx < len(audios):
                    aud_path = replace_media_path(audios[audio_idx], source_dir, target_dir)
                    if load_media:
                        if media_to_base64:
                            aud64 = load_audio_bytes_base64(aud_path)
                            content.append({"type": "input_audio", "audio_url": {"url": f"data:audio/ogg;base64,{aud64}"}})
                        else:
                            content.append({"type": "input_audio", "input_audio": {"url": aud_path}})
                    audio_idx += 1
                last_idx = end
            if last_idx < len(conv_text):
                content.append({"type": "text", "text": conv_text[last_idx:]})
            messages.append({"role": role, "content": content})
        elif role == 'system':
            content = [{"type": "text", "text": conv_text}]
            messages.append({"role": role, "content": content})
        elif role == 'assistant':
            if idx == len(conversations) - 1:
                answer = conv_text
            else:
                content = [{"type": "text", "text": conv_text}]
                messages.append({"role": role, "content": content})
        else:
            raise ValueError(f"Unknown role: {role}")
    if merge_answer:
        messages.append({"role": "assistant", "content": [{"type": "text", "text": answer}]})
        return messages
    else:
        return messages, answer

def load_media_item(path: str, media_type: str, load_media: bool, media_to_base64: bool):
    """
    è¿”å›å¤šåª’ä½“å†…å®¹ dict
    """
    if not load_media:
        return None
    
    if media_to_base64:
        if media_type == "image":
            data, img_type = load_image_base64_with_type(path)
            return {"type": "image_url", "image_url": {"url": f"data:{img_type};base64,{data}"}} 
        elif media_type == "video":
            data = load_video_bytes_base64(path)
            return {"type": "video_url", "video_url": {"url": f"data:video/mp4;base64,{data}"}} 
        elif media_type == "audio":
            data = load_audio_bytes_base64(path)
            return {"type": "input_audio", "audio_url": {"url": f"data:audio/ogg;base64,{data}"}} 
    else:
        # ä¸è½¬base64ï¼Œç›´æ¥è¿”å›è·¯å¾„
        if media_type == "image":
            return {"type": "image_url", "image_url": {"url": path}} 
        elif media_type == "video":
            return {"type": "video_url", "video_url": {"url": path}} 
        elif media_type == "audio":
            return {"type": "input_audio", "audio_url": {"url": path}} 


def remove_media_tags(text):
    return re.sub(r'<(image|video|audio)>', '', text)



def build_conversations(prompt: str,
                        response: str,
                        history: list[str, str],
                        train_history: bool):
    conversations = []
    for ques, answ in history:
        if_train = train_history
        conversations.append({"role": "user", "text": ques})
        conversations.append({"role": "assistant", "text": answ, "if_train": if_train})

    conversations.append({"role": "user", "text": prompt})
    conversations.append({"role": "assistant", "text": response, "if_train": True})

    return conversations


def test_hook(item, **kwargs):
    message, answer = load_message_from_data(item)
    print(message)
    yield item


def process_turn_hook(dataset_path,
                      turn_func,
                      prepare_initial_tasks_func,
                      dst_metafiles_name=None,
                      skip_processed_items=True,
                      num_workers=1,
                      **hook_kwargs):
    """
    å¤šè½®å¯¹è¯ä¸“ç”¨å¤„ç†å‡½æ•°ï¼Œæ”¯æŒè½®æ¬¡çº§åˆ«çš„åŠ¨æ€å¹¶è¡Œ
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆyamlæˆ–jsonlï¼‰
        turn_func: å•è½®å¤„ç†å‡½æ•°
        prepare_initial_tasks_func: å‡†å¤‡åˆå§‹ä»»åŠ¡å‡½æ•°
        dst_metafiles_name: ç›®æ ‡æ–‡ä»¶å¤¹åç§°
        skip_processed_items: æ˜¯å¦è·³è¿‡å·²å¤„ç†æ•°æ®
        num_workers: workeræ•°é‡
    """

    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"Dataset file {dataset_path} not found.")
    
    ext = os.path.splitext(dataset_path)[-1].lower()
    
    @dynamic_task_pool_multiprocess
    def _turn_wrapper(task, **kwargs):
        """åŒ…è£…turn_func"""
        return turn_func(task, **kwargs)
    
    initial_tasks = []
    
    if ext in ['.yaml', '.yml']:
        dataset_config = OmegaConf.load(dataset_path)
        
        for dataset_name, config in dataset_config["Datasets"].items():
            metafile_dir = config["MetaFiles"]
            if not os.path.exists(metafile_dir):
                log(f"Skip non-existent metafile_dir: {metafile_dir}")
                continue
                
            all_metafiles = glob(os.path.join(metafile_dir, "**", "*.jsonl"), recursive=True)
            
            if dst_metafiles_name == os.path.basename(metafile_dir):
                log(f"{dst_metafiles_name} conflicts with source, please use a different name.", level=log.ERROR)
                return
            
            for metafile in all_metafiles:
                try:
                    with open(metafile, 'r', encoding='utf-8') as f:
                        has_valid_line = False
                        for line in f:
                            if line.strip():
                                has_valid_line = True
                                break
                    if not has_valid_line:
                        log(f"Skip empty metafile: {metafile}")
                        continue
                except Exception as e:
                    log(f"Skip invalid metafile: {metafile}, error: {e}")
                    continue
                
                target_metafile = metafile.replace(
                    metafile_dir,
                    os.path.join(os.path.dirname(metafile_dir), dst_metafiles_name)
                )
                
                processed_items = set()
                if os.path.exists(target_metafile):
                    if skip_processed_items:
                        try:
                            processed_items = {item["id"] for item in load_jsonlines(target_metafile)}
                            log(f"Found {len(processed_items)} processed items in {target_metafile}")
                        except Exception as e:
                            log(f"Error loading processed items from {target_metafile}: {e}")
                    else:
                        # ğŸ”§ å½“ä¸è·³è¿‡å·²å¤„ç†é¡¹æ—¶ï¼Œåˆ é™¤å·²å­˜åœ¨çš„ç›®æ ‡æ–‡ä»¶
                        log(f"[skip_processed_items=False] Deleting existing target file: {target_metafile}")
                        try:
                            os.remove(target_metafile)
                            log(f"âœ… Deleted: {target_metafile}")
                        except Exception as e:
                            log(f"âš ï¸ Failed to delete {target_metafile}: {e}", level=log.WARNING)
                
                for item in load_jsonlines(metafile):
                    if item["id"] not in processed_items:
                        try:
                            initial_task = prepare_initial_tasks_func(
                                item,
                                src_path=metafile,
                                save_path=target_metafile,
                                config=config,
                                dataset_name=dataset_name,
                                **hook_kwargs
                            )
                            initial_tasks.append(initial_task)
                        except Exception as e:
                            log(f"Error preparing task for item {item.get('id', 'unknown')}: {e}", level=log.WARNING)
    else:
        raise ValueError(f"Unsupported file format: {ext}")
    
    log(f"Prepared {len(initial_tasks)} initial tasks from {dataset_path}")
    
    if not initial_tasks:
        log("No tasks to process.")
        return
    
    results = _turn_wrapper(initial_tasks, num_workers=num_workers, **hook_kwargs)
    
    # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„æ•°é‡
    # æŒ‰item_idåˆ†ç»„ï¼Œæ‰¾å‡ºæ¯ä¸ªitemçš„æœ€åä¸€è½®ç»“æœ
    item_results = {}
    for result in results:
        item_id = result.get('item_id')
        turn = result.get('turn', 0)
        
        # ä¿ç•™æ¯ä¸ªitemçš„æœ€å¤§è½®æ¬¡ï¼ˆæœ€åä¸€è½®ï¼‰
        if item_id not in item_results or turn > item_results[item_id]['turn']:
            item_results[item_id] = result
    
    # ç»Ÿè®¡æœ€åä¸€è½®çš„æˆåŠŸ/å¤±è´¥
    successful_items = set()
    failed_items = set()
    for item_id, last_result in item_results.items():
        if last_result.get('result') is not None:
            successful_items.add(item_id)
        else:
            failed_items.add(item_id)
    
    log("=" * 80)
    log(f"æ‰€æœ‰è½®æ¬¡å¤„ç†å®Œæˆï¼")
    log(f"æ€»ä»»åŠ¡æ•°: {len(initial_tasks)}")
    log(f"å®é™…å¤„ç†çš„items: {len(item_results)}")
    log(f"æˆåŠŸçš„items: {len(successful_items)}")
    log(f"å¤±è´¥çš„items: {len(failed_items)}")
    log(f"æœªå¤„ç†çš„items: {len(initial_tasks) - len(item_results)}")
    if failed_items and len(failed_items) <= 20:
        log(f"å¤±è´¥çš„item IDs: {sorted(list(failed_items))[:20]}")
    log("=" * 80)
    
    # åˆ é™¤ä¸´æ—¶é”æ–‡ä»¶
    log("Cleaning up lock files...")
    if ext in ['.yaml', '.yml']:
        dataset_config = OmegaConf.load(dataset_path)
        for dataset_name, config in dataset_config["Datasets"].items():
            metafile_dir = config["MetaFiles"]
            target_dir = os.path.join(os.path.dirname(metafile_dir), dst_metafiles_name)
            if os.path.exists(target_dir):
                lock_files = glob(os.path.join(target_dir, "**", "*.lock"), recursive=True)
                for lock_file in lock_files:
                    try:
                        if os.path.exists(lock_file):
                            os.remove(lock_file)
                    except Exception as e:
                        log(f"Failed to remove lock file {lock_file}: {e}", level=log.WARNING)
                log(f"Removed {len(lock_files)} lock files from {target_dir}")
    
    log("Done.")